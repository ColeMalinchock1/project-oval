{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26a3108-3953-4e45-af6e-e3e4e0921ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cell\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from eval_helper import calculate_mAP\n",
    "from torchvision import models\n",
    "from torch.utils.data import random_split\n",
    "from cocodataset import Compose, ToTensor, CocoDataset\n",
    "import sys\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda30ca4-256a-4bca-90fc-254d3f02dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loader, device, epoch, lr_scheduler):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    all_losses = []\n",
    "    all_losses_dict = []\n",
    "    \n",
    "    for images, targets in tqdm(loader):\n",
    "        # Skip batches with None\n",
    "        if images is None or targets is None or any(img is None for img in images) or any(tar is None for tar in targets):\n",
    "            continue\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        all_losses_dict.append(loss_dict_append)\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "    all_losses_dict = pd.DataFrame(all_losses_dict)\n",
    "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
    "        all_losses_dict['loss_classifier'].mean(),\n",
    "        all_losses_dict['loss_box_reg'].mean(),\n",
    "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
    "        all_losses_dict['loss_objectness'].mean()\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630321c6-a7cd-4eb6-8afd-359b519b447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_loader, model, device, label_threshold=0.8, iou_threshold=0.5):\n",
    "    device1 = torch.device('cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    det_boxes = []\n",
    "    det_labels = []\n",
    "    det_scores = []\n",
    "    true_boxes = []\n",
    "    true_labels = []\n",
    "    true_difficulties = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader):\n",
    "            if images is None or targets is None or any(img is None for img in images) or any(tar is None for tar in targets):\n",
    "                continue\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            predictions = model(images)\n",
    "            for i in range(len(predictions)):\n",
    "                pred = predictions[i]\n",
    "                det_boxes.append(pred['boxes'][pred['scores'] > label_threshold].to(device1))\n",
    "                det_labels.append(pred['labels'][pred['scores'] > label_threshold].to(device1))\n",
    "                det_scores.append(torch.tensor([sc for sc in pred['scores'].tolist() if sc > label_threshold], dtype=torch.float32).to(device1))\n",
    "                true_boxes.append(targets[i]['boxes'].to(device1))\n",
    "                true_labels.append(targets[i]['labels'].to(device1))\n",
    "                true_difficulties.append(torch.zeros(len(targets[i]['labels'])).to(device1))   \n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties, device1, iou_threshold)\n",
    "    return APs, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a308b7aa-3a23-4e5d-bc74-9d5b3dfe2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    n_classes = 2\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def get_data_loader(img_folder, ann_file, train_flag=True):\n",
    "    transforms = Compose([ToTensor()])\n",
    "    dataset = CocoDataset(img_folder, ann_file, transforms=transforms)\n",
    "    batch_size = 4\n",
    "    workers = 4\n",
    "    if not train_flag:\n",
    "        workers = 1 \n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train_flag, num_workers=workers,\n",
    "                                                pin_memory=True, collate_fn=collate_fn)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf8bd31-4f0a-4f0d-ba83-db1c7e279194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python script_name.py <train_img_folder> <train_ann_file> <val_img_folder> <val_ann_file>\n"
     ]
    }
   ],
   "source": [
    "def main(train_img_folder, train_ann_file, val_img_folder, val_ann_file):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    model = get_model()\n",
    "    train_loader = get_data_loader(train_img_folder, train_ann_file, train_flag=True)\n",
    "    val_loader = get_data_loader(val_img_folder, val_ann_file, train_flag=False)\n",
    "\n",
    "    print(\"Number of GPUs: \" + str(n_gpu))\n",
    "\n",
    "    train_losses = []\n",
    "    n_epochs = 3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = None\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        train_one_epoch(model, optimizer, train_loader, device, epoch, scheduler)\n",
    "        print(evaluate(val_loader, model, device))\n",
    "        model.eval()\n",
    "        torch.save(model.state_dict(), 'model_epoch' + str(epoch) + '.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 5:\n",
    "        print(\"Usage: python script_name.py <train_img_folder> <train_ann_file> <val_img_folder> <val_ann_file>\")\n",
    "    else:\n",
    "        main(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089b7b3f-abd3-44b4-b179-885be709fd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training images:\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 4102/4102 [00:00<00:00, 249898.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images are found.\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_image_files(img_folder, ann_file):\n",
    "    coco = COCO(ann_file)\n",
    "    img_ids = coco.getImgIds()\n",
    "    missing_images = []\n",
    "    missing_image_ids = []\n",
    "\n",
    "    for img_id in tqdm(img_ids):\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_file = os.path.join(img_folder, img_info['file_name'])\n",
    "        if not os.path.exists(img_file):\n",
    "            print(f\"Image not found: {img_file}\")\n",
    "            missing_images.append(img_file)\n",
    "            missing_image_ids.append(img_id)\n",
    "    \n",
    "    if not missing_images:\n",
    "        print(\"All images are found.\")\n",
    "    else:\n",
    "        print(f\"Total missing images: {len(missing_images)}\")\n",
    "    return missing_images, missing_image_ids\n",
    "\n",
    "train_img_folder = \"./building_structures.v1i.coco/train/images\"\n",
    "train_ann_file = \"./building_structures.v1i.coco/train/_annotations.coco.json\"\n",
    "val_img_folder = \"./building_structures.v1i.coco/valid/images\"\n",
    "val_ann_file = \"./building_structures.v1i.coco/valid/_annotations.coco.json\"\n",
    "\n",
    "print(\"Checking training images:\")\n",
    "img_not_found, img_ids_not_found = check_image_files(train_img_folder, train_ann_file)\n",
    "print(img_not_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d762580-b628-427d-b06e-14c864f1418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated annotations saved to ./building_structures.v1i.coco/train/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Delete the annotation whose no mapped image\n",
    "def delete_annotations(ann_file, img_ids_not_found):\n",
    "    with open(ann_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    annotations['images'] = [img for img in annotations['images'] if img['id'] not in img_ids_not_found]\n",
    "    annotations['annotations'] = [ann for ann in annotations['annotations'] if ann['image_id'] not in img_ids_not_found]\n",
    "\n",
    "    with open(ann_file, 'w') as f:\n",
    "        json.dump(annotations, f, indent=4)\n",
    "    \n",
    "    print(f\"Updated annotations saved to {ann_file}\")\n",
    "\n",
    "delete_annotations(train_ann_file, img_ids_not_found)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36bed0-ae71-43c7-92d6-953d501713e2",
   "metadata": {},
   "source": [
    "Checking training images:\n",
    "\n",
    "\n",
    "***Image not found: ./building_structures.v1i.coco/train/images/dunrobin-zamok-sazerlend-shotlandiya-alba-velikobritaniya-zamok-park-solnce-svet-nebo-oblaka-zelen-trava-gazon-kusty-derevya-fontan-priroda-pejzazh_jpg.rf.3831b7c324e2aeefdf0a8e12abeaa82d.jpg***\n",
    "\n",
    "Total missing images: 1\n",
    "\n",
    "Checking validation images:\n",
    "\n",
    "All images are found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a53e83-1225-4fdb-b7f6-71d86216ead2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pthapa4/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/pthapa4/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of GPUs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1026 [00:00<?, ?it/s]/tmp/ipykernel_49010/1928987733.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
      " 65%|██████████████████████████▏             | 672/1026 [01:17<00:40,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No bounding boxes found for idx 1609. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████▎       | 828/1026 [01:35<00:22,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No bounding boxes found for idx 1392. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1026/1026 [01:58<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, lr: 0.000100, loss: 0.137503, loss_classifier: 0.047648, loss_box: 0.075964, loss_rpn_box: 0.006096, loss_object: 0.007795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 35/35 [00:01<00:00, 19.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'1': 0.9004670977592468, '2': 0.0, '3': 0.0, '4': 0.0}, 0.2251167744398117)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████▌           | 732/1026 [01:23<00:33,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No bounding boxes found for idx 1392. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████▊       | 842/1026 [01:36<00:21,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No bounding boxes found for idx 1609. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1026/1026 [01:57<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr: 0.000100, loss: 0.113840, loss_classifier: 0.039041, loss_box: 0.065693, loss_rpn_box: 0.005051, loss_object: 0.004055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 35/35 [00:01<00:00, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'1': 0.8970125317573547, '2': 0.0, '3': 0.0, '4': 0.0}, 0.22425313293933868)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████▎                      | 443/1026 [00:50<01:07,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No bounding boxes found for idx 1392. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████████████████████▍            | 704/1026 [01:20<00:36,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No bounding boxes found for idx 1609. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1026/1026 [01:57<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, lr: 0.000100, loss: 0.103175, loss_classifier: 0.034826, loss_box: 0.060598, loss_rpn_box: 0.004681, loss_object: 0.003070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 35/35 [00:01<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'1': 0.896914541721344, '2': 0.0, '3': 0.0, '4': 0.0}, 0.224228635430336)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████▎                   | 520/1026 [00:59<00:57,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No bounding boxes found for idx 1609. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████▎                | 599/1026 [01:08<00:50,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No bounding boxes found for idx 1392. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1026/1026 [01:57<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, lr: 0.000100, loss: 0.088174, loss_classifier: 0.028392, loss_box: 0.053234, loss_rpn_box: 0.004281, loss_object: 0.002267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 35/35 [00:01<00:00, 19.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'1': 0.9014368057250977, '2': 0.0, '3': 0.0, '4': 0.0}, 0.22535920143127441)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_img_folder = \"./building_structures.v1i.coco/train/images\"\n",
    "train_ann_file = \"./building_structures.v1i.coco/train/_annotations.coco.json\"\n",
    "val_img_folder = \"./building_structures.v1i.coco/valid/images\"\n",
    "val_ann_file = \"./building_structures.v1i.coco/valid/_annotations.coco.json\"\n",
    "\n",
    "main(train_img_folder, train_ann_file, val_img_folder, val_ann_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a41c5bf-0b87-484f-8ab1-c6293b7e15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Eval \n",
    "def load_model(checkpoint_path, n_classes=2):\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78050499-3d03-4265-ab42-f587197ed425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Evaluating model: model_epoch0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 35/35 [00:01<00:00, 19.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_epoch0.pth, mAP: 0.2251167744398117\n",
      "Label: 1, AP: 0.9004670977592468\n",
      "Label: 2, AP: 0.0\n",
      "Label: 3, AP: 0.0\n",
      "Label: 4, AP: 0.0\n",
      "Evaluating model: model_epoch1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 35/35 [00:01<00:00, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_epoch1.pth, mAP: 0.22425313293933868\n",
      "Label: 1, AP: 0.8970125317573547\n",
      "Label: 2, AP: 0.0\n",
      "Label: 3, AP: 0.0\n",
      "Label: 4, AP: 0.0\n",
      "Evaluating model: model_epoch2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 35/35 [00:01<00:00, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_epoch2.pth, mAP: 0.224228635430336\n",
      "Label: 1, AP: 0.896914541721344\n",
      "Label: 2, AP: 0.0\n",
      "Label: 3, AP: 0.0\n",
      "Label: 4, AP: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_data_loader(img_folder, ann_file, train_flag=True):\n",
    "    transforms = Compose([ToTensor()])\n",
    "    dataset = CocoDataset(img_folder, ann_file, transforms=transforms)\n",
    "    batch_size = 4\n",
    "    workers = 4\n",
    "    if not train_flag:\n",
    "        workers = 1\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train_flag, num_workers=workers,\n",
    "                                              pin_memory=True, collate_fn=collate_fn)\n",
    "    return data_loader\n",
    "\n",
    "def evaluate_models(model_paths, val_img_folder, val_ann_file, device):\n",
    "    val_loader = get_data_loader(val_img_folder, val_ann_file, train_flag=False)\n",
    "\n",
    "    for model_path in model_paths:\n",
    "        print(f\"Evaluating model: {model_path}\")\n",
    "        model = load_model(model_path)\n",
    "        APs, mAP = evaluate(val_loader, model, device)\n",
    "        print(f\"Model: {model_path}, mAP: {mAP}\")\n",
    "        for label, ap in APs.items():\n",
    "            print(f\"Label: {label}, AP: {ap}\")\n",
    "\n",
    "# Example usage\n",
    "model_paths = ['model_epoch0.pth', 'model_epoch1.pth', 'model_epoch2.pth']\n",
    "val_img_folder = \"./building_structures.v1i.coco/valid/images\"\n",
    "val_ann_file = \"./building_structures.v1i.coco/valid/_annotations.coco.json\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "evaluate_models(model_paths, val_img_folder, val_ann_file, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae737c5-f178-4a9f-a07e-2104aef005b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./building_structures.v1i.coco/valid/images/2048x1365_915257_-www-ArtFile-ru-_jpg.rf.7ba5fcc6756d006ae4221490e010985d.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m predict_and_draw(model, image_path)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(checkpoint_path, n_classes)\u001b[0m\n\u001b[1;32m      4\u001b[0m in_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor\u001b[38;5;241m.\u001b[39mcls_score\u001b[38;5;241m.\u001b[39min_features\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mdetection\u001b[38;5;241m.\u001b[39mfaster_rcnn\u001b[38;5;241m.\u001b[39mFastRCNNPredictor(in_features, n_classes)\n\u001b[0;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/project-oval/lib/python3.8/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/project-oval/lib/python3.8/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/project-oval/lib/python3.8/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/project-oval/lib/python3.8/site-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/project-oval/lib/python3.8/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/project-oval/lib/python3.8/site-packages/torch/serialization.py:265\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 265\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/anaconda3/envs/project-oval/lib/python3.8/site-packages/torch/serialization.py:249\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    246\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    254\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# Function to load and transform the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Function to draw bounding boxes on the image\n",
    "def draw_boxes(image_path, boxes, labels, scores, threshold=0.5):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= threshold:\n",
    "            draw.rectangle(((box[0], box[1]), (box[2], box[3])), outline=\"red\", width=3)\n",
    "            draw.text((box[0], box[1]), f\"Label: {label}, Score: {score:.2f}\", fill=\"red\")\n",
    "    return image\n",
    "\n",
    "# Function to make predictions and draw bounding boxes\n",
    "def predict_and_draw(model, device, image_path, threshold=0.5):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and transform the image\n",
    "    image_tensor = load_image(image_path).to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "\n",
    "    # Get boxes, labels, and scores\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "    # Draw bounding boxes on the image\n",
    "    image_with_boxes = draw_boxes(image_path, boxes, labels, scores, threshold)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_with_boxes)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "model_path = 'model_epoch3.pth'\n",
    "image_path = './building_structures.v1i.coco/valid/images/2048x1365_915257_-www-ArtFile-ru-_jpg.rf.7ba5fcc6756d006ae4221490e010985d.jpg'  # Replace with your image path\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "predict_and_draw(model, device, image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
