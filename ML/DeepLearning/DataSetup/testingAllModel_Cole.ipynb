{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24da1e8c-359c-476f-bbd9-c57068f9da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from pycocotools.coco import COCO\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import patches\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn.functional as F  \n",
    "import torch.nn as nn\n",
    "import shutil  # Used for copying files\n",
    "from torchvision.transforms import ToTensor\n",
    "from ultralytics import YOLO\n",
    "from torchvision.transforms import ToTensor, Resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d847acd5-f513-44aa-a8a5-525676adb764",
   "metadata": {},
   "source": [
    "# Open the csv file, get all the image name and let the faster RCNN predict what the class it is and stuff\n",
    "# Create a coco dataset with all the images and bounding box\n",
    "# Use the images and bounding box to predict the lat lon for different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c438a660-84c7-453f-8161-a30dc6644ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is required for visualization of the images with bounding box\n",
    "# class COCODataset(Dataset):\n",
    "#     def __init__(self, annotation_file, image_dir, transforms=None):\n",
    "#         self.coco = COCO(annotation_file)\n",
    "#         self.image_dir = image_dir\n",
    "#         self.transforms = transforms\n",
    "#         self.ids = list(self.coco.imgs.keys())  # List of image IDs\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # Get the image ID and load the associated annotations\n",
    "#         img_id = self.ids[index]\n",
    "#         ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "#         anns = self.coco.loadAnns(ann_ids)\n",
    "#         image_info = self.coco.loadImgs(img_id)[0]\n",
    "#         path = image_info['file_name']\n",
    "\n",
    "#         # Load the image using OpenCV\n",
    "#         img = cv2.imread(os.path.join(self.image_dir, path))\n",
    "        \n",
    "#         if img is None:\n",
    "#             raise FileNotFoundError(f\"Image not found at path: {img_path}\")\n",
    "\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         num_objs = len(anns)\n",
    "#         boxes = []\n",
    "#         labels = []\n",
    "        \n",
    "#         # Extract the bounding boxes and category labels\n",
    "#         for i in range(num_objs):\n",
    "#             xmin = anns[i]['bbox'][0]\n",
    "#             ymin = anns[i]['bbox'][1]\n",
    "#             xmax = xmin + anns[i]['bbox'][2]\n",
    "#             ymax = ymin + anns[i]['bbox'][3]\n",
    "#             boxes.append([xmin, ymin, xmax, ymax])\n",
    "#             labels.append(anns[i]['category_id'])\n",
    "\n",
    "#         # Convert boxes and labels to tensors\n",
    "#         boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "#         if boxes.ndim == 1:\n",
    "#             boxes = boxes.unsqueeze(0)\n",
    "#         labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "#         image_id = torch.tensor([img_id])\n",
    "        \n",
    "#         # Calculate the area of the boxes\n",
    "#         if boxes.size(0) > 0:  # Check if there are any boxes\n",
    "#             area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "#         else:\n",
    "#             area = torch.tensor([])\n",
    "\n",
    "#         # Set the crowd flag to 0 (no crowd annotations in this case)\n",
    "#         iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "#         # Build the target dictionary\n",
    "#         target = {}\n",
    "#         target[\"boxes\"] = boxes\n",
    "#         target[\"labels\"] = labels\n",
    "#         target[\"image_id\"] = image_id\n",
    "#         target[\"area\"] = area\n",
    "#         target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "#         # Apply transformations if specified\n",
    "#         if self.transforms:\n",
    "#             img = self.transforms(img)\n",
    "\n",
    "#         return img, target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ids)\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, coco_file, images_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco_annotations = self.load_coco_annotations(coco_file)\n",
    "        self.bounding_boxes, self.image_files = self.process_data(self.coco_annotations)\n",
    "\n",
    "    def load_coco_annotations(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "        return coco_data\n",
    "\n",
    "    def process_data(self, coco_data):\n",
    "        max_instances_per_class = 2\n",
    "        num_classes = 9\n",
    "        data_points = 4\n",
    "        input_size = num_classes * max_instances_per_class * data_points\n",
    "        bounding_boxes = []\n",
    "        image_files = []\n",
    "\n",
    "        for image_info in coco_data['images']:\n",
    "            image_id = image_info['id']\n",
    "\n",
    "            input_vector = [0] * input_size\n",
    "            annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_id]\n",
    "            for ann in annotations:\n",
    "                class_id = ann['category_id']\n",
    "                bbox = ann['bbox']\n",
    "                instance_index = sum(1 for a in annotations if a['category_id'] == class_id) - 1\n",
    "                if instance_index < max_instances_per_class:\n",
    "                    start_index = (class_id * max_instances_per_class + instance_index) * 4\n",
    "                    length = 1280.0\n",
    "                    width = 720.0\n",
    "                    bbox[0] = bbox[0] / length\n",
    "                    bbox[1] = bbox[1] / width\n",
    "                    bbox[2] = bbox[2] / length\n",
    "                    bbox[3] = bbox[3] / width\n",
    "                    input_vector[start_index:start_index + 4] = bbox\n",
    "\n",
    "            bounding_boxes.append(input_vector)\n",
    "            image_files.append(image_info['file_name'])\n",
    "\n",
    "        return bounding_boxes, image_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bounding_boxes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bounding_box = torch.tensor(self.bounding_boxes[idx], dtype=torch.float32)\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_file)\n",
    "        return bounding_box, image_path\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, yolo_file, images_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.yolo_annotations = self.load_yolo_annotations(yolo_file)\n",
    "        self.bounding_boxes, self.image_files = self.process_data(self.yolo_annotations)\n",
    "\n",
    "    def load_yolo_annotations(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            yolo_data = json.load(f)\n",
    "        return yolo_data\n",
    "\n",
    "    def process_data(self, yolo_data):\n",
    "        max_instances_per_class = 2\n",
    "        num_classes = 9\n",
    "        data_points = 4\n",
    "        input_size = num_classes * max_instances_per_class * data_points\n",
    "        bounding_boxes = []\n",
    "        image_files = []\n",
    "\n",
    "        for image_info in yolo_data['images']:\n",
    "            image_id = image_info['id']\n",
    "\n",
    "            input_vector = [0] * input_size\n",
    "            annotations = [ann for ann in yolo_data['annotations'] if ann['image_id'] == image_id]\n",
    "            for ann in annotations:\n",
    "                class_id = ann['category_id']\n",
    "                bbox = ann['bbox']\n",
    "                instance_index = sum(1 for a in annotations if a['category_id'] == class_id) - 1\n",
    "                if instance_index < max_instances_per_class:\n",
    "                    start_index = (class_id * max_instances_per_class + instance_index) * 4\n",
    "                    length = 1280.0\n",
    "                    width = 720.0\n",
    "                    bbox[0] = bbox[0] / length\n",
    "                    bbox[1] = bbox[1] / width\n",
    "                    bbox[2] = bbox[2] / length\n",
    "                    bbox[3] = bbox[3] / width\n",
    "                    input_vector[start_index:start_index + 4] = bbox\n",
    "\n",
    "            bounding_boxes.append(input_vector)\n",
    "            image_files.append(image_info['file_name'])\n",
    "\n",
    "        return bounding_boxes, image_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bounding_boxes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bounding_box = torch.tensor(self.bounding_boxes[idx], dtype=torch.float32)\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_file)\n",
    "        return bounding_box, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2af9a8f-62d6-47cd-9f01-9eb17db35152",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_21_10.jpg: 384x640 1 street-lamp, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_35_08.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_20.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_06_19.jpg: 384x640 1 engineering-building, 2 trashcans, 5.1ms\n",
      "Speed: 0.9ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_13_35.jpg: 384x640 3 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_48.jpg: 384x640 (no detections), 2.2ms\n",
      "Speed: 0.6ms preprocess, 2.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_48.jpg: 384x640 1 FW, 2 signs, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_52_17.jpg: 384x640 2 engineering-buildings, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_21_45.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_10.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_58_11.jpg: 384x640 1 engineering-building, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_06.jpg: 384x640 2 OVALs, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_07_43.jpg: 384x640 1 FW, 1 HUNT, 1 engineering-building, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_58_19.jpg: 384x640 2 engineering-buildings, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_15_06.jpg: 384x640 1 engineering-building, 1 sign, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_22.jpg: 384x640 1 FW, 1 engineering-building, 2 street-lamps, 2 trashcans, 1.9ms\n",
      "Speed: 0.8ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_30.jpg: 384x640 3 OVALs, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_30_40.jpg: 384x640 1 engineering-building, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_35_10.jpg: 384x640 1 sign, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_50_34.jpg: 384x640 1 FW, 2 engineering-buildings, 1 street-lamp, 2.2ms\n",
      "Speed: 0.7ms preprocess, 2.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_36_12.jpg: 384x640 2 engineering-buildings, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_41_44.jpg: 384x640 1 FW, 1 engineering-building, 3 signs, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_27.jpg: 384x640 1 FW, 2 HUNTs, 1 street-lamp, 2.1ms\n",
      "Speed: 0.7ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_19.jpg: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_59_58.jpg: 384x640 1 engineering-building, 2.2ms\n",
      "Speed: 0.7ms preprocess, 2.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_22.jpg: 384x640 1 engineering-building, 1 street-lamp, 2 trashcans, 2.2ms\n",
      "Speed: 0.7ms preprocess, 2.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_55.jpg: 384x640 3 engineering-buildings, 2.4ms\n",
      "Speed: 0.8ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_16.jpg: 384x640 1 OVAL, 1 street-lamp, 3.3ms\n",
      "Speed: 0.9ms preprocess, 3.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_03_16.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_57.jpg: 384x640 3 engineering-buildings, 1 street-lamp, 2.1ms\n",
      "Speed: 0.6ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_55.jpg: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_49.jpg: 384x640 1 FW, 1 OVAL, 1 street-lamp, 2.1ms\n",
      "Speed: 0.7ms preprocess, 2.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_15.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_46_25.jpg: 384x640 1 FW, 1 OVAL, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_35_04.jpg: 384x640 1 OVAL, 2.2ms\n",
      "Speed: 0.7ms preprocess, 2.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_29.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1 trashcan, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_39_13.jpg: 384x640 1 HUNT, 1 OVAL, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_31.jpg: 384x640 2 OVALs, 1 trashcan, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_46.jpg: 384x640 1 FW, 1 HUNT, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_08.jpg: 384x640 1 FW, 2 signs, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_40_52.jpg: 384x640 1 FW, 2 OVALs, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_45.jpg: 384x640 4 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_32.jpg: 384x640 2 engineering-buildings, 1 trashcan, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_42.jpg: 384x640 1 FW, 1 HUNT, 2 street-lamps, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_06.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 2.2ms\n",
      "Speed: 0.7ms preprocess, 2.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_36_31.jpg: 384x640 2 FWs, 2 engineering-buildings, 1 sign, 2.1ms\n",
      "Speed: 0.7ms preprocess, 2.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_47.jpg: 384x640 1 FW, 2 street-lamps, 2.1ms\n",
      "Speed: 0.7ms preprocess, 2.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_03.jpg: 384x640 2 engineering-buildings, 2 signs, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_36_27.jpg: 384x640 1 sign, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_11.jpg: 384x640 1 FW, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_56.jpg: 384x640 1 HUNT, 1 OVAL, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_14_14.jpg: 384x640 1 engineering-building, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_08_29.jpg: 384x640 1 FW, 1 engineering-building, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_59_25.jpg: 384x640 5 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_35_56.jpg: 384x640 1 engineering-building, 1 security-station, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_25_24.jpg: 384x640 1 FW, 1 OVAL, 2 street-lamps, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_13.jpg: 384x640 1 engineering-building, 1 sign, 1 street-lamp, 2.1ms\n",
      "Speed: 0.7ms preprocess, 2.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_46_29.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 3.8ms\n",
      "Speed: 0.9ms preprocess, 3.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_15_12.jpg: 384x640 1 engineering-building, 1 street-lamp, 2.1ms\n",
      "Speed: 0.7ms preprocess, 2.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_46_11.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_15.jpg: 384x640 1 FW, 1 sign, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_36_22.jpg: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_35_15.jpg: 384x640 2 engineering-buildings, 1 sign, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_10_43.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_34_13.jpg: 384x640 1 FW, 2.1ms\n",
      "Speed: 0.7ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_29_11.jpg: 384x640 1 sign, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_30_00.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_14.jpg: 384x640 1 FW, 1 HUNT, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_18_03.jpg: 384x640 1 FW, 1 HUNT, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_58_48.jpg: 384x640 2 FWs, 3 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_37.jpg: 384x640 1 HUNT, 1.9ms\n",
      "Speed: 0.8ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_14_18.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_49_14.jpg: 384x640 1 FW, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_25.jpg: 384x640 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_26_07.jpg: 384x640 1 FW, 1 sign, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_36_35.jpg: 384x640 1 engineering-building, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_37_53.jpg: 384x640 2 engineering-buildings, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_39_25.jpg: 384x640 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_16_09.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_57_34.jpg: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_31_46.jpg: 384x640 1 street-lamp, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_59_47.jpg: 384x640 1 engineering-building, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_15_41.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_43_04.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_54_45.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_00.jpg: 384x640 1 HUNT, 1 OVAL, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_52_42.jpg: 384x640 1 OVAL, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_39.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_14_37.jpg: 384x640 1 engineering-building, 1 street-lamp, 5.1ms\n",
      "Speed: 0.9ms preprocess, 5.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_04.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.8ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_50_59.jpg: 384x640 1 sign, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_45.jpg: 384x640 1 OVAL, 1 security-station, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_12_52.jpg: 384x640 2 engineering-buildings, 1 sign, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_59_08.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_32_16.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_59_33.jpg: 384x640 4 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_31_31.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_17.jpg: 384x640 1 HUNT, 2 OVALs, 1 security-station, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_22_33.jpg: 384x640 1 FW, 1 engineering-building, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_39_13.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_22_25.jpg: 384x640 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_25_03.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_41.jpg: 384x640 1 FW, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_00_29.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_35.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_38_43.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_31_33.jpg: 384x640 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_35.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_56.jpg: 384x640 1 FW, 1 security-station, 1 sign, 4.1ms\n",
      "Speed: 0.9ms preprocess, 4.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_23_46.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_39.jpg: 384x640 3 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_59_27.jpg: 384x640 4 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_34.jpg: 384x640 1 engineering-building, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_23.jpg: 384x640 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_31.jpg: 384x640 2 engineering-buildings, 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_39_27.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_37.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_31_37.jpg: 384x640 1 HUNT, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_35_47.jpg: 384x640 2 engineering-buildings, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_56.jpg: 384x640 2 engineering-buildings, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_31_43.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1 engineering-building, 2.1ms\n",
      "Speed: 0.7ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_10_28.jpg: 384x640 1 FW, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_07.jpg: 384x640 1 FW, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_29.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_08_14.jpg: 384x640 1 HUNT, 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_55_33.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_39_07.jpg: 384x640 1 HUNT, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_37.jpg: 384x640 3 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_27.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_22.jpg: 384x640 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_36.jpg: 384x640 3 OVALs, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_37.jpg: 384x640 1 FW, 1 engineering-building, 4.9ms\n",
      "Speed: 0.9ms preprocess, 4.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_17_58.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_45_40.jpg: 384x640 1 FW, 2 engineering-buildings, 3 trashcans, 2.3ms\n",
      "Speed: 0.9ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_49_07.jpg: 384x640 1 FW, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_16_03.jpg: 384x640 2 engineering-buildings, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_23.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_19.jpg: 384x640 1 engineering-building, 1 security-station, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_31_34.jpg: 384x640 1 HUNT, 1 street-lamp, 3.6ms\n",
      "Speed: 0.9ms preprocess, 3.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_27_39.jpg: 384x640 1 street-lamp, 4.9ms\n",
      "Speed: 0.8ms preprocess, 4.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_47.jpg: 384x640 1 FW, 1 engineering-building, 2.1ms\n",
      "Speed: 0.6ms preprocess, 2.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_40_13.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_41_21.jpg: 384x640 1 FW, 2 street-lamps, 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_15.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_30.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_47.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_12_40.jpg: 384x640 2 FWs, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_20.jpg: 384x640 1 FW, 1 trashcan, 2.7ms\n",
      "Speed: 0.7ms preprocess, 2.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_30_19.jpg: 384x640 1 engineering-building, 1 trashcan, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_08.jpg: 384x640 3 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_28_28.jpg: 384x640 1 FW, 2 engineering-buildings, 1 security-station, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_03_36.jpg: 384x640 2 FWs, 1 street-lamp, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_05.jpg: 384x640 1 HUNT, 1 engineering-building, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_05.jpg: 384x640 2 engineering-buildings, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_41.jpg: 384x640 1 HUNT, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_52.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_18_13.jpg: 384x640 1 FW, 2 trashcans, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_54_31.jpg: 384x640 1 FW, 1 HUNT, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_39_23.jpg: 384x640 2 HUNTs, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_46.jpg: 384x640 1 OVAL, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_05.jpg: 384x640 1 engineering-building, 1 sign, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_41_29.jpg: 384x640 1 FW, 3 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_26_25.jpg: 384x640 1 FW, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_06.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_00_08.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_32.jpg: 384x640 2 OVALs, 1 street-lamp, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_00.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_12.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_12.jpg: 384x640 3 engineering-buildings, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_25_08.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_17_32.jpg: 384x640 1 HUNT, 1 engineering-building, 1.7ms\n",
      "Speed: 0.6ms preprocess, 1.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_29_07.jpg: 384x640 2 engineering-buildings, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_23_19.jpg: 384x640 2 engineering-buildings, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_12.jpg: 384x640 1 FW, 2.1ms\n",
      "Speed: 0.6ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_13_45.jpg: 384x640 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_52_07.jpg: 384x640 4 engineering-buildings, 2.1ms\n",
      "Speed: 0.6ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_33_07.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_28_11.jpg: 384x640 1 HUNT, 2 engineering-buildings, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_26_17.jpg: 384x640 1 FW, 1 sign, 1 trashcan, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_21_57.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_17.jpg: 384x640 1 FW, 1 HUNT, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_06_14.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_07_02.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_41_48.jpg: 384x640 1 FW, 1 HUNT, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_31.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_17_19.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_21_22.jpg: 384x640 1 HUNT, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_17_44.jpg: 384x640 1 HUNT, 1 street-lamp, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_30_58.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_08_06.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 2.1ms\n",
      "Speed: 0.6ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_43.jpg: 384x640 4 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_35.jpg: 384x640 3 engineering-buildings, 1 street-lamp, 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_36_02.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_31_06.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_29_19.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_38.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_28.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_52_23.jpg: 384x640 3 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_59.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_30.jpg: 384x640 2 security-stations, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_54.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_35_20.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_55_45.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_40_00.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_50.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_02.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.9ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_21_35.jpg: 384x640 3 engineering-buildings, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_24.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_33.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_50.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_21_29.jpg: 384x640 1 HUNT, 3 engineering-buildings, 1.8ms\n",
      "Speed: 0.7ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_30_55.jpg: 384x640 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_22_56.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_30_33.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_22_44.jpg: 384x640 1 HUNT, 1 engineering-building, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_22.jpg: 384x640 1 OVAL, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_40_19.jpg: 384x640 1 FW, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_31_12.jpg: 384x640 1 FW, 2 HUNTs, 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_40.jpg: 384x640 1 OVAL, 1 security-station, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_52_05.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_54.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_35.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_59.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_16.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_54.jpg: 384x640 1 FW, 1 security-station, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_02.jpg: 384x640 1 engineering-building, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_32.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_36_49.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_08.jpg: 384x640 1 OVAL, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_14_04.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_35_17.jpg: 384x640 1 engineering-building, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_58.jpg: 384x640 1 security-station, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_27_27.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_43_24.jpg: 384x640 1 engineering-building, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_06_37.jpg: 384x640 2 trashcans, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_49_34.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_39_07.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_41_41.jpg: 384x640 1 FW, 1 security-station, 2 signs, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_17_09.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_23_33.jpg: 384x640 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_20.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_18_50.jpg: 384x640 2 FWs, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_33.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_15_26.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.8ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_28_40.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_32.jpg: 384x640 1 OVAL, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_06_51.jpg: 384x640 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_13_27.jpg: 384x640 3 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_32_15.jpg: 384x640 1 FW, 1 engineering-building, 1 security-station, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_38.jpg: 384x640 1 FW, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_07_37.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_25_21.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_06.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_33_15.jpg: 384x640 1 FW, 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_38.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_03_55.jpg: 384x640 1 FW, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_31.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_54_25.jpg: 384x640 1 FW, 1 HUNT, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_57_07.jpg: 384x640 1 OVAL, 1 sign, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_04.jpg: 384x640 1 HUNT, 1 street-lamp, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_50_07.jpg: 384x640 1 FW, 2 engineering-buildings, 1 sign, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_23_38.jpg: 384x640 2 OVALs, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_26.jpg: 384x640 1 OVAL, 2 engineering-buildings, 1 security-station, 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_23.jpg: 384x640 3 engineering-buildings, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_52.jpg: 384x640 1 OVAL, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_30_25.jpg: 384x640 1 HUNT, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_45.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_31_36.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_49_26.jpg: 384x640 1 FW, 1 sign, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_36_47.jpg: 384x640 1 engineering-building, 2 street-lamps, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_39.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_25_00.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_12_38.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_57_36.jpg: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_51.jpg: 384x640 1 FW, 1 street-lamp, 3 trashcans, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_34_05.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_33.jpg: 384x640 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_15_28.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_10_24.jpg: 384x640 1 FW, 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_00.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_29.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_01.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_16.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1.8ms\n",
      "Speed: 0.7ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_18.jpg: 384x640 1 engineering-building, 1 security-station, 2 street-lamps, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_46_46.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_26_27.jpg: 384x640 1 FW, 1 HUNT, 2 signs, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_08.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_34.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_09.jpg: 384x640 7 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_00.jpg: 384x640 1 HUNT, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_55_35.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_00_06.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_03_03.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_33_03.jpg: 384x640 3 FWs, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_58.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_10.jpg: 384x640 2 FWs, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_55_20.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_22.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_02.jpg: 384x640 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_27_02.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_59.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_55.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_50_44.jpg: 384x640 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_06_27.jpg: 384x640 4 engineering-buildings, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_52_40.jpg: 384x640 1 OVAL, 1 engineering-building, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_32_25.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_19_56.jpg: 384x640 1 FW, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_00_18.jpg: 384x640 1 HUNT, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_49_46.jpg: 384x640 2 FWs, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_28.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_54_37.jpg: 384x640 2 FWs, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_12_54.jpg: 384x640 1 FW, 1 engineering-building, 1 sign, 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_02.jpg: 384x640 1 HUNT, 1 OVAL, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_33_24.jpg: 384x640 1 FW, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_49.jpg: 384x640 3 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_12.jpg: 384x640 3 engineering-buildings, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_20.jpg: 384x640 3 OVALs, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_50.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_29.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_18_31.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_01.jpg: 384x640 1 engineering-building, 1 security-station, 1 trashcan, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_22_58.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_27_43.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_31.jpg: 384x640 1 FW, 1 HUNT, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_17_01.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_50.jpg: 384x640 1 FW, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_27.jpg: 384x640 1 OVAL, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_28_09.jpg: 384x640 1 FW, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_27_41.jpg: 384x640 1 engineering-building, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_32_47.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_41.jpg: 384x640 1 OVAL, 2 engineering-buildings, 1 sign, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_57_46.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_35_19.jpg: 384x640 1 HUNT, 1 engineering-building, 1 sign, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_00.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_49.jpg: 384x640 1 FW, 1 engineering-building, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_21_00.jpg: 384x640 1 sign, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_10.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_40.jpg: 384x640 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_49.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_53.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_46_21.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_22.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_06_08.jpg: 384x640 2 engineering-buildings, 1.8ms\n",
      "Speed: 0.7ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_23_52.jpg: 384x640 1 HUNT, 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_54.jpg: 384x640 1 FW, 1 HUNT, 2 street-lamps, 1.8ms\n",
      "Speed: 0.7ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_39_09.jpg: 384x640 1 HUNT, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_18.jpg: 384x640 3 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_41.jpg: 384x640 4 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_00_14.jpg: 384x640 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_18.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_10.jpg: 384x640 1 OVAL, 1 security-station, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_33.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_32_08.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_27_47.jpg: 384x640 1 engineering-building, 1 sign, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_04.jpg: 384x640 1 FW, 1 engineering-building, 3 trashcans, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_47.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_55.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_32_45.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_02.jpg: 384x640 1 FW, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_17_15.jpg: 384x640 2 FWs, 1 HUNT, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_12.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_01.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.7ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_40.jpg: 384x640 1 FW, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_25_26.jpg: 384x640 1 FW, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_55.jpg: 384x640 1 OVAL, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_06.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_25_43.jpg: 384x640 1 FW, 1 security-station, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_14_31.jpg: 384x640 2 engineering-buildings, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_14.jpg: 384x640 3 engineering-buildings, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_40.jpg: 384x640 3 OVALs, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_18.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_08_20.jpg: 384x640 1 FW, 1 HUNT, 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_29.jpg: 384x640 2 OVALs, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_25_06.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_39_46.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_45_42.jpg: 384x640 1 HUNT, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_28.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 2 trashcans, 1.8ms\n",
      "Speed: 0.7ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_08.jpg: 384x640 1 FW, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_32_58.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.5ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_27.jpg: 384x640 1 FW, 1 HUNT, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_14_06.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_51.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_49_05.jpg: 384x640 1 FW, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_31_13.jpg: 384x640 1 FW, 1 HUNT, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_24.jpg: 384x640 1 security-station, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_07_18.jpg: 384x640 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_20.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_57.jpg: 384x640 1 HUNT, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_17.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_17_38.jpg: 384x640 1 HUNT, 2 OVALs, 1 security-station, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_32_00.jpg: 384x640 1 FW, 2 HUNTs, 2.1ms\n",
      "Speed: 0.6ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_14.jpg: 384x640 2 OVALs, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_38_01.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_13_54.jpg: 384x640 6 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_07_58.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_13_02.jpg: 384x640 3 engineering-buildings, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_29_40.jpg: 384x640 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_28_42.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_13_56.jpg: 384x640 6 engineering-buildings, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_27_17.jpg: 384x640 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_07_04.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_18.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_35_52.jpg: 384x640 1 engineering-building, 1 street-lamp, 2 trashcans, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_08_45.jpg: 384x640 1 HUNT, 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_33_38.jpg: 384x640 1 FW, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_30_36.jpg: 384x640 3 engineering-buildings, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_28_05.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_38_41.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_54.jpg: 384x640 1 OVAL, 1 security-station, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_19.jpg: 384x640 2 FWs, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_18.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.5ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_21_33.jpg: 384x640 3 engineering-buildings, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_59.jpg: 384x640 3 engineering-buildings, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_18.jpg: 384x640 2 OVALs, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_56.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_47.jpg: 384x640 1 HUNT, 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_34.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_55_43.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_58_25.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_59_35.jpg: 384x640 2 engineering-buildings, 4.9ms\n",
      "Speed: 0.9ms preprocess, 4.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_20.jpg: 384x640 3 engineering-buildings, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_40_56.jpg: 384x640 1 FW, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_38_07.jpg: 384x640 1 HUNT, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_36.jpg: 384x640 1 FW, 1 HUNT, 2 signs, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_12_09.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_26.jpg: 384x640 1 FW, 1 security-station, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_16_30.jpg: 384x640 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_57_56.jpg: 384x640 1 engineering-building, 1 security-station, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_30_52.jpg: 384x640 1 FW, 1 HUNT, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_35_57.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_49_18.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_45_34.jpg: 384x640 1 engineering-building, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_43.jpg: 384x640 1 FW, 1 security-station, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_19_46.jpg: 384x640 1 FW, 2 street-lamps, 3 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_04.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_36.jpg: 384x640 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_38_12.jpg: 384x640 1 HUNT, 1 OVAL, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_16.jpg: 384x640 4 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_08_56.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_34_33.jpg: 384x640 1 security-station, 1 trashcan, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_09.jpg: 384x640 1 HUNT, 1 OVAL, 1 sign, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_03_40.jpg: 384x640 1 FW, 1 OVAL, 1 street-lamp, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_40_58.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_14.jpg: 384x640 1 OVAL, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_22_39.jpg: 384x640 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_45.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_13_00.jpg: 384x640 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_33_01.jpg: 384x640 2 FWs, 3 trashcans, 1.8ms\n",
      "Speed: 0.5ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_27_23.jpg: 384x640 2 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_31_50.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_40_42.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_37.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_08.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_58_42.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_40.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_15_37.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_28.jpg: 384x640 2 OVALs, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_27_12.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_55_08.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_34_50.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_26_24.jpg: 384x640 1 OVAL, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_25_30.jpg: 384x640 1 FW, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_36.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_34.jpg: 384x640 1 HUNT, 3 engineering-buildings, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_25.jpg: 384x640 1 FW, 2 HUNTs, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_18.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_18_52.jpg: 384x640 2 FWs, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_41.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_55_51.jpg: 384x640 1 HUNT, 1 security-station, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_23_39.jpg: 384x640 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_18_48.jpg: 384x640 1 FW, 1 HUNT, 1 sign, 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_54_33.jpg: 384x640 1 FW, 1 HUNT, 2.1ms\n",
      "Speed: 0.6ms preprocess, 2.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_36_53.jpg: 384x640 2 engineering-buildings, 1 security-station, 1 street-lamp, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_30_43.jpg: 384x640 1 HUNT, 1 OVAL, 1 street-lamp, 1.9ms\n",
      "Speed: 0.8ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_25_10.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_14.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_59_19.jpg: 384x640 4 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_38_55.jpg: 384x640 1 FW, 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_35.jpg: 384x640 1 OVAL, 2.1ms\n",
      "Speed: 0.7ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_31_08.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_28_34.jpg: 384x640 1 engineering-building, 1 security-station, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_50.jpg: 384x640 3 engineering-buildings, 1 sign, 1 street-lamp, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_58.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_11_36.jpg: 384x640 3 OVALs, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_06_56.jpg: 384x640 2 engineering-buildings, 2 street-lamps, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_55.jpg: 384x640 3 engineering-buildings, 2 street-lamps, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_03_30.jpg: 384x640 2 FWs, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_49.jpg: 384x640 1 FW, 1 street-lamp, 1 trashcan, 2.1ms\n",
      "Speed: 0.6ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_57.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_25.jpg: 384x640 2 engineering-buildings, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_50_28.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_29_54.jpg: 384x640 3 engineering-buildings, 1 security-station, 2 street-lamps, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_10.jpg: 384x640 1 FW, 1 security-station, 1 street-lamp, 1.8ms\n",
      "Speed: 0.5ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_25_52.jpg: 384x640 1 FW, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_35_00.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_40_21.jpg: 384x640 1 FW, 1 security-station, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_38_57.jpg: 384x640 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_19_06.jpg: 384x640 1 FW, 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_08_24.jpg: 384x640 1 HUNT, 1 OVAL, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_00_33.jpg: 384x640 1 FW, 1 HUNT, 1 sign, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_55.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_45_52.jpg: 384x640 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_22.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_03_34.jpg: 384x640 2 FWs, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_30_57.jpg: 384x640 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_32_04.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_43.jpg: 384x640 1 HUNT, 1 security-station, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_57_30.jpg: 384x640 1 OVAL, 1.8ms\n",
      "Speed: 0.7ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_33_09.jpg: 384x640 2 FWs, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_10_16.jpg: 384x640 1 FW, 2 street-lamps, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_31_17.jpg: 384x640 1 FW, 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_03.jpg: 384x640 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_31_29.jpg: 384x640 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_32_56.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_03_57.jpg: 384x640 1 FW, 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_18_29.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 2.1ms\n",
      "Speed: 0.6ms preprocess, 2.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_20.jpg: 384x640 2 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_57.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_31_00.jpg: 384x640 1 FW, 1 HUNT, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_45_56.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_00_20.jpg: 384x640 1 HUNT, 1 OVAL, 1 security-station, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_59_54.jpg: 384x640 1 OVAL, 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_06_02.jpg: 384x640 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_31_07.jpg: 384x640 1 FW, 1 HUNT, 1 OVAL, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_24_09.jpg: 384x640 1 FW, 1 HUNT, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_17.jpg: 384x640 1 FW, 1 HUNT, 2 security-stations, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_10_34.jpg: 384x640 1 FW, 1.8ms\n",
      "Speed: 0.5ms preprocess, 1.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_27_55.jpg: 384x640 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_42.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_39_44.jpg: 384x640 1 FW, 1 HUNT, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_30_59.jpg: 384x640 1 HUNT, 1 OVAL, 3 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_56.jpg: 384x640 1 OVAL, 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_50_57.jpg: 384x640 2 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_32_31.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_32_54.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.7ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_35.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_04.jpg: 384x640 1 FW, 1 HUNT, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_53.jpg: 384x640 2 OVALs, 1 security-station, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_53_19.jpg: 384x640 2 engineering-buildings, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_05_13.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_56_38.jpg: 384x640 2 OVALs, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_07_45.jpg: 384x640 1 HUNT, 4 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_10_26.jpg: 384x640 1 FW, 2 street-lamps, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_41_17.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_35_30.jpg: 384x640 (no detections), 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_14_45.jpg: 384x640 1 engineering-building, 1 street-lamp, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_11.jpg: 384x640 1 engineering-building, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_36_00.jpg: 384x640 1 street-lamp, 2 trashcans, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_29_15.jpg: 384x640 2 engineering-buildings, 1 sign, 1.8ms\n",
      "Speed: 0.5ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_57_13.jpg: 384x640 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_32_22.jpg: 384x640 1 FW, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_51_36.jpg: 384x640 1 engineering-building, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_26_09.jpg: 384x640 1 FW, 1 sign, 1 street-lamp, 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_38_38.jpg: 384x640 3 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_31_52.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_28.jpg: 384x640 1 street-lamp, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_46_54.jpg: 384x640 1 HUNT, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_49.jpg: 384x640 1 HUNT, 3 engineering-buildings, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_18_25.jpg: 384x640 1 FW, 1 HUNT, 1 security-station, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_14.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_33_52.jpg: 384x640 1 FW, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_22_52.jpg: 384x640 2 engineering-buildings, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_38_47.jpg: 384x640 2 FWs, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_16_44.jpg: 384x640 1 FW, 1 security-station, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_12_48.jpg: 384x640 1 FW, 2 engineering-buildings, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_19_54.jpg: 384x640 1 FW, 1 engineering-building, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_24_02.jpg: 384x640 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_35_27.jpg: 384x640 2 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_34.jpg: 384x640 1 FW, 1 HUNT, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_30_23.jpg: 384x640 1 FW, 1 HUNT, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_53.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_32.jpg: 384x640 2 FWs, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_15_22.jpg: 384x640 1 engineering-building, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_03_07.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_02_26.jpg: 384x640 1 FW, 1 HUNT, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_37_39.jpg: 384x640 1 sign, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_23_07.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.5ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_39_01.jpg: 384x640 (no detections), 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_32_28.jpg: 384x640 1 FW, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_25_33.jpg: 384x640 1 HUNT, 1.8ms\n",
      "Speed: 0.5ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_54_43.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.5ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_25_50.jpg: 384x640 1 FW, 1 street-lamp, 1.8ms\n",
      "Speed: 0.7ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_37_14.jpg: 384x640 4 engineering-buildings, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_34_59.jpg: 384x640 1 OVAL, 1 security-station, 1 sign, 2.0ms\n",
      "Speed: 0.6ms preprocess, 2.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_55_26.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_07_16.jpg: 384x640 1 FW, 2 street-lamps, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_55_10.jpg: 384x640 1 FW, 1 HUNT, 1 trashcan, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_20_52.jpg: 384x640 1 engineering-building, 1 sign, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_35.jpg: 384x640 1 FW, 1 HUNT, 2.0ms\n",
      "Speed: 0.7ms preprocess, 2.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_33_02.jpg: 384x640 1 FW, 1 HUNT, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_29_09.jpg: 384x640 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_47_10.jpg: 384x640 1 HUNT, 1 OVAL, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_42.jpg: 384x640 1 FW, 1 sign, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_04_26.jpg: 384x640 1 FW, 1 sign, 1 trashcan, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_42_24.jpg: 384x640 1 OVAL, 1 street-lamp, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_12_19.jpg: 384x640 3 engineering-buildings, 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_01_57.jpg: 384x640 1 HUNT, 2 trashcans, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_28_22.jpg: 384x640 2 engineering-buildings, 2 trashcans, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_10_18.jpg: 384x640 1 FW, 1 OVAL, 1 street-lamp, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_48_37.jpg: 384x640 2 FWs, 1 trashcan, 1.8ms\n",
      "Speed: 0.5ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-15_46_58.jpg: 384x640 1 HUNT, 1 engineering-building, 1.9ms\n",
      "Speed: 0.6ms preprocess, 1.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/chmalinc/GitHub/project-oval/ML/DeepLearning/DataSetup/captured_images_test7/2024-09-08-16_09_39.jpg: 384x640 1 FW, 1 HUNT, 1.8ms\n",
      "Speed: 0.6ms preprocess, 1.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo_annotations.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Process the images and create JSON file\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 25\u001b[0m, in \u001b[0;36mprocess_images\u001b[0;34m(image_dir, output_json)\u001b[0m\n\u001b[1;32m     22\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Run YOLOv8 prediction\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Add image information\u001b[39;00m\n\u001b[1;32m     28\u001b[0m images\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_id,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_file\n\u001b[1;32m     31\u001b[0m })\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:101\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:242\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:196\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:259\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 259\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/predictor.py:135\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m visualize \u001b[38;5;241m=\u001b[39m increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem,\n\u001b[1;32m    134\u001b[0m                            mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:347\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    344\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize) \u001b[38;5;28;01mif\u001b[39;00m augment \u001b[38;5;129;01mor\u001b[39;00m visualize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:42\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:59\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/tasks.py:79\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 79\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m     80\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/nn/modules/head.py:45\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m shape \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# BCHW\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[0;32m---> 45\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2[i](x[i]), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv3\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# USES THE YOLO MODEL TO CREATE A JSON FILE\n",
    "\n",
    "import os\n",
    "import json\n",
    "from ultralytics import YOLO  # Assuming you are using the Ultralytics YOLOv8 package\n",
    "import cv2\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO(\"yolo.pt\")\n",
    "\n",
    "# Define a function to process images and extract predictions\n",
    "def process_images(image_dir, output_json):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    image_id = 1\n",
    "    annotation_id = 1\n",
    "    \n",
    "    # Loop through each image in the directory\n",
    "    for image_file in os.listdir(image_dir):\n",
    "        if image_file.endswith(\".jpg\") or image_file.endswith(\".png\"):  # Check for image files\n",
    "            image_path = os.path.join(image_dir, image_file)\n",
    "            img = cv2.imread(image_path)\n",
    "\n",
    "            # Run YOLOv8 prediction\n",
    "            results = model(image_path)\n",
    "\n",
    "            # Add image information\n",
    "            images.append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": image_file\n",
    "            })\n",
    "\n",
    "            # Loop through each prediction for the current image\n",
    "            for result in results[0].boxes:\n",
    "                # Extract bounding box, category, and score\n",
    "                bbox = result.xyxy[0].tolist()  # YOLOv8 gives the bounding box as [x_min, y_min, x_max, y_max]\n",
    "                category_id = int(result.cls) + 1  # Assuming category IDs start from 1\n",
    "                score = float(result.conf)  # Confidence score\n",
    "\n",
    "                # Convert bbox to COCO format (x_min, y_min, width, height)\n",
    "                bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]\n",
    "\n",
    "                # Add annotation information\n",
    "                annotations.append({\n",
    "                    \"id\": annotation_id + 1,\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": category_id,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"score\": score\n",
    "                })\n",
    "                annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "\n",
    "    # Prepare final JSON structure\n",
    "    output_data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations\n",
    "    }\n",
    "\n",
    "    # Write the output to a JSON file\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "\n",
    "# Directory containing your images\n",
    "image_directory = \"captured_images_test7\"\n",
    "\n",
    "# Output JSON file\n",
    "output_file = \"yolo_annotations.json\"\n",
    "\n",
    "# Process the images and create JSON file\n",
    "process_images(image_directory, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5484890-5350-4f9e-b99d-417ea9ce1977",
   "metadata": {},
   "source": [
    "# Model (FFNN, LSTM) Architecture for Lat and Lon Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6091bc2-4052-4107-b740-01dd557abc10",
   "metadata": {},
   "source": [
    "# FFNN Architecture with first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf9a3896-ffbe-46c2-bb51-76dafbd7781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatLonModelFFNNWith3Digits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LatLonModelFFNNWith3Digits, self).__init__()\n",
    "        self.fc1 = nn.Linear(72, 40)\n",
    "        self.bn1 = nn.BatchNorm1d(40)\n",
    "        self.fc2 = nn.Linear(40, 2)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116a801-c685-4ea0-8da9-54e24c98a6f4",
   "metadata": {},
   "source": [
    "# FFNN Architecture without first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c2e327b-9126-4113-9426-195be0c6a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatLonModelFFNNWithOut3Digits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LatLonModelFFNNWithOut3Digits, self).__init__()\n",
    "        self.fc1 = nn.Linear(72, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.fc5 = nn.Linear(32, 2)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f42c1-d341-4d56-a9a5-6b12223e1afb",
   "metadata": {},
   "source": [
    "# LSTM Architecture with first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90b755e7-1550-433c-9728-aee6b69b0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatLonModelLSTMWith3digits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LatLonModelLSTMWith3digits, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=72, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        # Fully connected layers with consistent sizes\n",
    "        self.fc1 = nn.Linear(256, 256)  # Keep the output size same as input for skip connection\n",
    "        self.fc2 = nn.Linear(256, 128)  # Reduce size in the second layer\n",
    "        self.fc3 = nn.Linear(128, 2)    # Final layer for output\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]  # Take the output of the last time step\n",
    "        \n",
    "        # First fully connected layer with skip connection\n",
    "        residual = x  # Save input for the skip connection\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x += residual  # Add skip connection\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second fully connected layer (no skip connection here)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d1850-c325-49d8-bdfc-6e208de4c398",
   "metadata": {},
   "source": [
    "# LSTM Architecture without first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b28dfa6-514e-4e3c-bd5d-eb2fb1377f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "class LatLonModelLSTMWithout3digits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LatLonModelLSTMWithout3digits, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=72, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 128)  # Reduced from 512 to 256\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]  # Take the output of the last time step\n",
    "        \n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79268aaf-14d9-4777-a134-314a4e168112",
   "metadata": {},
   "source": [
    "# Get hold of data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d588a9-b8a6-4de0-b095-e5dd402c9ac2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_7/position_data_logger.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m corrected_image \n\u001b[1;32m     23\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_7/position_data_logger.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m corrected_image \u001b[38;5;241m=\u001b[39m \u001b[43mopen_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corrected_image))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(corrected_image[:\u001b[38;5;241m20\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mopen_csv\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_csv\u001b[39m(filepath):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Opens CSV, returns modified filenames and corresponding swift lat/lon.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure correct encoding\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     corrected_image \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m     swift_data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_7/position_data_logger.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to open the CSV file, correct the image names, and extract swift lat/lon data\n",
    "def open_csv(filepath):\n",
    "    \"\"\"Opens CSV, returns modified filenames and corresponding swift lat/lon.\"\"\"\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')  # Ensure correct encoding\n",
    "\n",
    "    corrected_image = []\n",
    "    swift_data = []\n",
    "    \n",
    "    # Making the correct format and extract swift_lat, swift_lon\n",
    "    for index, row in df.iterrows():\n",
    "        image = row[\"image_name\"]\n",
    "        \n",
    "        if isinstance(image, str):  # Ensure the value is a string\n",
    "            corrected_img_name = image.replace(\":\", \"_\").strip()  # Replace \":\" and remove leading/trailing spaces\n",
    "            corrected_image.append((corrected_img_name, row[\"swift_latitude\"], row[\"swift_longitude\"]))\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return corrected_image \n",
    "\n",
    "csv_path = \"test_7/position_data_logger.csv\"\n",
    "corrected_image = open_csv(csv_path)\n",
    "print(len(corrected_image))\n",
    "print(corrected_image[:20])\n",
    "print(isinstance(corrected_image, list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb24bf41-79e3-442a-985b-288e57bda42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n",
      "[('2024-09-08-15_22_15.jpg', 35.7711512322439, -78.6739819342967), ('2024-09-08-15_22_17.jpg', 35.7711342470924, -78.6739971917349), ('2024-09-08-15_22_19.jpg', 35.7711140432787, -78.6740133320289), ('2024-09-08-15_22_21.jpg', 35.7710938628983, -78.674033581905), ('2024-09-08-15_22_23.jpg', 35.7710737387602, -78.6740491115901), ('2024-09-08-15_22_25.jpg', 35.7710553816594, -78.6740621245691), ('2024-09-08-15_22_27.jpg', 35.7710342310763, -78.6740784431818), ('2024-09-08-15_22_29.jpg', 35.7710142073795, -78.6740935657712), ('2024-09-08-15_22_31.jpg', 35.7709934078664, -78.6741091332326), ('2024-09-08-15_22_33.jpg', 35.7709738044288, -78.6741257466179), ('2024-09-08-15_22_35.jpg', 35.770952349122, -78.6741423858114), ('2024-09-08-15_22_37.jpg', 35.7709299054234, -78.6741589816041), ('2024-09-08-15_22_39.jpg', 35.7709047461554, -78.6741750122562), ('2024-09-08-15_22_41.jpg', 35.7708848766937, -78.6741895761486), ('2024-09-08-15_22_44.jpg', 35.7708634880776, -78.6742050178157), ('2024-09-08-15_22_46.jpg', 35.7708453140304, -78.674219163195), ('2024-09-08-15_22_48.jpg', 35.7708291175321, -78.6742327112112), ('2024-09-08-15_22_50.jpg', 35.7708067825815, -78.6742493217248), ('2024-09-08-15_22_52.jpg', 35.7707838692627, -78.6742663662356), ('2024-09-08-15_22_54.jpg', 35.7707652577157, -78.6742812856374)]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "found_image = []\n",
    "not_found_image = []\n",
    "folder_path = \"test_7/captured_images_test7\"\n",
    "for filename in corrected_image:\n",
    "    file, lat, lon = filename\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "        not_found_image.append(filename)\n",
    "    else:\n",
    "        found_image.append(filename)\n",
    "        \n",
    "print(len(found_image))\n",
    "print(found_image[:20])\n",
    "print(len(not_found_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283bf41-e441-43c1-8711-eded31d23cb0",
   "metadata": {},
   "source": [
    "# Creates a COCODataset using Faster RCNN for detecting 9 classes (sign, FW, HUNT, OVAL, engineering-building, security-station, sign, street-lamp, trashcan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4150b75-4461-4ef9-8d07-31c709ccefc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating COCO-style annotations.\n"
     ]
    }
   ],
   "source": [
    "# Load the Faster RCNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one (for 9 classes + background)\n",
    "num_classes = 9 + 1  # 9 classes + background\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Load the model state dict (from previously saved model)\n",
    "model_save_path = \"faster_rcnn.pth\"\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Categories for COCO format\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"sign\"},\n",
    "    {\"id\": 1, \"name\": \"FW\"},\n",
    "    {\"id\": 2, \"name\": \"HUNT\"},\n",
    "    {\"id\": 3, \"name\": \"OVAL\"},\n",
    "    {\"id\": 4, \"name\": \"engineering-building\"},\n",
    "    {\"id\": 5, \"name\": \"security-station\"},\n",
    "    {\"id\": 6, \"name\": \"sign\"},\n",
    "    {\"id\": 7, \"name\": \"street-lamp\"},\n",
    "    {\"id\": 8, \"name\": \"trashcan\"}\n",
    "]\n",
    "\n",
    "# ['sign', 'FW', 'HUNT', 'Oval', 'engineering-building', 'security-station', 'sign', 'street-lamp', 'trashcan']\n",
    "def load_image(file_path):\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    transform = ToTensor()  # Create a ToTensor transformation\n",
    "    return transform(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "\n",
    "# Function to annotate images in COCO format\n",
    "def annotate_images_in_folder(found_images, folder_path, output_json_path, threshold=0.5):\n",
    "    image_id = 1\n",
    "    annotation_id = 1\n",
    "    coco_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    # print(found_images)\n",
    "    # Loop through found images, checking if they exist in the folder\n",
    "    for image_info in found_images:\n",
    "        filename, swift_latitude, swift_longitude = image_info\n",
    "        # print(filename)\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            # print(f\"Image not found: {filename}\")\n",
    "            continue  # Skip if the image is not found\n",
    "\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Load and preprocess the image\n",
    "            image = load_image(file_path).to(device)\n",
    "\n",
    "            # Predict using Faster R-CNN\n",
    "            with torch.no_grad():\n",
    "                prediction = model(image)\n",
    "\n",
    "            # Get image dimensions\n",
    "            image_np = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "            height, width, _ = image_np.shape\n",
    "\n",
    "            # Append image details to COCO annotations\n",
    "            coco_annotations[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": filename,\n",
    "                \"width\": int(width),\n",
    "                \"height\": int(height),\n",
    "                \"swift_latitude\": float(swift_latitude),  # Add swift latitude\n",
    "                \"swift_longitude\": float(swift_longitude)  # Add swift longitude\n",
    "            })\n",
    "\n",
    "            # Get the predictions\n",
    "            boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "            scores = prediction[0]['scores'].cpu().numpy()\n",
    "            labels = prediction[0]['labels'].cpu().numpy()\n",
    "\n",
    "            # Create a list of (score, label, box) tuples and sort by score descending\n",
    "            predictions = sorted(zip(scores, labels, boxes), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Keep track of the count of predictions per label type (to limit to 2 per class)\n",
    "            label_counts = {}\n",
    "\n",
    "            # Add top 2 predictions for each class\n",
    "            for score, label, box in predictions:\n",
    "                if score >= threshold:\n",
    "                    if label not in label_counts:\n",
    "                        label_counts[label] = 0\n",
    "                    if label_counts[label] < 2:\n",
    "                        xmin, ymin, xmax, ymax = box\n",
    "                        width = xmax - xmin\n",
    "                        height = ymax - ymin\n",
    "                        coco_annotations[\"annotations\"].append({\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": int(label),\n",
    "                            \"bbox\": [float(xmin), float(ymin), float(width), float(height)],\n",
    "                            \"score\": float(score)\n",
    "                        })\n",
    "                        label_counts[label] += 1\n",
    "                        annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "\n",
    "    # Save the annotations to a JSON file\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(coco_annotations, f, indent=4)\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'test_7/captured_images_test7'  # The folder where the images are stored\n",
    "output_json_path = 'test_7/RCNN_BBOX_WITH_LAT_LON.json'\n",
    "\n",
    "# Annotate images and create a COCO-style dataset\n",
    "annotate_images_in_folder(corrected_image, folder_path, output_json_path)\n",
    "print(\"Finished creating COCO-style annotations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02995e-1f2f-4859-9ac8-2c2af7cdd234",
   "metadata": {},
   "source": [
    "# Creates a COCODataset using Yolo Model for detecting 9 classes ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b1b9b8-b987-4eec-8b3f-90325f8c9147",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOLO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m(model_path)\n\u001b[1;32m      3\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_image1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Warming up the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YOLO' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = \"best.pt\"\n",
    "yolo_model = YOLO(model_path)\n",
    "test_image = \"test_image1.jpg\"\n",
    "# Warming up the model\n",
    "_ = yolo_model(test_image)\n",
    "CUSTOM_CLASS_NAMES = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "def plot_detections(image_path, results):\n",
    "    # Load the image using OpenCV but use Matplotlib for display\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for Matplotlib\n",
    "    \n",
    "    # Create a Matplotlib figure and axis\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image_rgb)\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy\n",
    "        confidences = result.boxes.conf\n",
    "        class_ids = result.boxes.cls\n",
    "\n",
    "        for box, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "            x_min, y_min, x_max, y_max = map(int, box)\n",
    "            label = CUSTOM_CLASS_NAMES[int(class_id)]\n",
    "            score = float(confidence)\n",
    "\n",
    "            # Create a Rectangle patch for bounding box\n",
    "            rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='green', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add label and score\n",
    "            label_text = f'{label}: {score:.2f}'\n",
    "            ax.text(x_min, y_min - 5, label_text, color='white', fontsize=12, bbox=dict(facecolor='green', alpha=0.5))\n",
    "\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "result = yolo_model(test_image)\n",
    "plot_detections(test_image, result)\n",
    "\n",
    "# Categories for COCO format\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"sign\"},\n",
    "    {\"id\": 1, \"name\": \"FW\"},\n",
    "    {\"id\": 2, \"name\": \"HUNT\"},\n",
    "    {\"id\": 3, \"name\": \"OVAL\"},\n",
    "    {\"id\": 4, \"name\": \"engineering-building\"},\n",
    "    {\"id\": 5, \"name\": \"security-station\"},\n",
    "    {\"id\": 6, \"name\": \"sign\"},\n",
    "    {\"id\": 7, \"name\": \"street-lamp\"},\n",
    "    {\"id\": 8, \"name\": \"trashcan\"}\n",
    "]\n",
    "\n",
    "# Function to load and resize image\n",
    "def load_image(file_path):\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    resize_transform = Resize((640, 640))  # Resize image to 640x640\n",
    "    transform = ToTensor()  # Create a ToTensor transformation\n",
    "    image = resize_transform(image)  # Resize the image\n",
    "    return transform(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "\n",
    "# Function to annotate images in COCO format\n",
    "def annotate_images_in_folder(found_images, folder_path, output_json_path, threshold=0.5):\n",
    "    image_id = 1\n",
    "    annotation_id = 1\n",
    "    coco_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories\n",
    "    }\n",
    "\n",
    "    # Loop through found images, checking if they exist in the folder\n",
    "    for image_info in found_images:\n",
    "        filename, swift_latitude, swift_longitude = image_info\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            continue  # Skip if the image is not found\n",
    "\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Load and preprocess the image\n",
    "            image = load_image(file_path).to(device)\n",
    "\n",
    "            # Predict using YOLO model\n",
    "            with torch.no_grad():\n",
    "                prediction = yolo_model(image)[0]  # Adjusting here to get the first prediction result\n",
    "\n",
    "            # Get image dimensions\n",
    "            image_np = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "            height, width, _ = image_np.shape\n",
    "\n",
    "            # Append image details to COCO annotations\n",
    "            coco_annotations[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": filename,\n",
    "                \"width\": int(width),\n",
    "                \"height\": int(height),\n",
    "                \"swift_latitude\": float(swift_latitude),  # Add swift latitude\n",
    "                \"swift_longitude\": float(swift_longitude)  # Add swift longitude\n",
    "            })\n",
    "\n",
    "            # Extract predictions\n",
    "            boxes = prediction.boxes.xyxy.cpu().numpy()  # Bounding boxes\n",
    "            scores = prediction.boxes.conf.cpu().numpy()  # Confidence scores\n",
    "            labels = prediction.boxes.cls.cpu().numpy()  # Class labels\n",
    "\n",
    "            # Create a list of (score, label, box) tuples and sort by score descending\n",
    "            predictions = sorted(zip(scores, labels, boxes), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Keep track of the count of predictions per label type (to limit to 2 per class)\n",
    "            label_counts = {}\n",
    "\n",
    "            # Add top 2 predictions for each class\n",
    "            for score, label, box in predictions:\n",
    "                if score >= threshold:\n",
    "                    if label not in label_counts:\n",
    "                        label_counts[label] = 0\n",
    "                    if label_counts[label] < 2:\n",
    "                        xmin, ymin, xmax, ymax = box\n",
    "                        width = xmax - xmin\n",
    "                        height = ymax - ymin\n",
    "                        coco_annotations[\"annotations\"].append({\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": int(label),\n",
    "                            \"bbox\": [float(xmin), float(ymin), float(width), float(height)],\n",
    "                            \"score\": float(score)\n",
    "                        })\n",
    "                        label_counts[label] += 1\n",
    "                        annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "\n",
    "    # Save the annotations to a JSON file\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(coco_annotations, f, indent=4)\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'test_5/captured_images'  # The folder where the images are stored\n",
    "output_json_path = 'test_5/YOLO_2BBox_Each_Class_Annotation.json'\n",
    "\n",
    "# Annotate images and create a COCO-style dataset\n",
    "annotate_images_in_folder(corrected_image, folder_path, output_json_path)\n",
    "print(\"Finished creating COCO-style annotations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93854fa1-97a5-4ff7-958c-d5dff2ebc10b",
   "metadata": {},
   "source": [
    "# Checking to make sure the model is predicting the bbox right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d221e687-f7b8-488d-a0f6-bb0f83ec4496",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Visualize the batch\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mvisualize_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mvisualize_batch\u001b[0;34m(images, targets)\u001b[0m\n\u001b[1;32m     10\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(images), figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (img, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, targets)):\n\u001b[0;32m---> 12\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Convert tensor to numpy array\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     img \u001b[38;5;241m=\u001b[39m (img \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)  \u001b[38;5;66;03m# Convert to uint8\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     axes[i]\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 3"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "annotation_file = 'yolo_annotations.json'\n",
    "image_dir = 'test_7/captured_images_test7'\n",
    "dataset = YOLODataset(annotation_file, image_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "# Function to visualize a batch of images\n",
    "def visualize_batch(images, targets):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(20, 5))\n",
    "    for i, (img, target) in enumerate(zip(images, targets)):\n",
    "        img = img.permute(1, 2, 0).numpy()  # Convert tensor to numpy array\n",
    "        img = (img * 255).astype(np.uint8)  # Convert to uint8\n",
    "        axes[i].imshow(img)\n",
    "        boxes = target['boxes'].numpy()\n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red')\n",
    "            axes[i].add_patch(rect)\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of data\n",
    "images, targets = next(iter(dataloader))\n",
    "\n",
    "# Visualize the batch\n",
    "visualize_batch(images, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6dfe3-0dff-47f3-8906-42ca7651f93a",
   "metadata": {},
   "source": [
    "# Loading LatLonModelWith3Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef7427af-37db-4f9a-a69d-a0bd2d086f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in the dataset: 2112\n",
      "Number of batches processed: 212\n",
      "Number of predictions made: 2112\n",
      "Total image paths processed: 212\n"
     ]
    }
   ],
   "source": [
    "lat_lon_model_with_3digits = LatLonModelFFNNWith3Digits()\n",
    "lat_lon_model_with_3digits.load_state_dict(torch.load('ffnn_location_no_shortened_digits.pt', weights_only=True))  # Load the saved weights\n",
    "lat_lon_model_with_3digits.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Making predictions and saving them\n",
    "annotation_file = 'yolo_annotations.json'\n",
    "found_image_dir = \"test_7/captured_images_test7\"\n",
    "lat_lon_dataset_with_3digits = YOLODataset(annotation_file, found_image_dir)\n",
    "lat_lon_test_dataloader = DataLoader(lat_lon_dataset_with_3digits, batch_size=10, shuffle=False)\n",
    "\n",
    "print(f\"Total images in the dataset: {len(lat_lon_dataset_with_3digits)}\")\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "# corrected_image = open_csv(csv_path)\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "image_paths = []  # Uncommented to track image paths\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in lat_lon_test_dataloader:\n",
    "        # Predict outputs\n",
    "        outputs = lat_lon_model_with_3digits(inputs)\n",
    "        \n",
    "        # print(outputs)\n",
    "        all_predictions.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        \n",
    "        image_paths.append(targets)  \n",
    "# print(all_predictions)\n",
    "flat_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()  # Converts to numpy array\n",
    "flat_image_paths = [path for batch in image_paths for path in batch]  # Flatten image paths if batched\n",
    "\n",
    "# Check if all images were processed\n",
    "print(f\"Number of batches processed: {len(all_predictions)}\")\n",
    "print(f\"Number of predictions made: {sum([len(batch) for batch in all_predictions])}\")\n",
    "\n",
    "data = {\n",
    "    \"image_path\" : flat_image_paths,\n",
    "    \"predicted_lat\" : flat_predictions[:, 0],\n",
    "    \"predicted_lon\": flat_predictions[:, 1]\n",
    "}\n",
    "\n",
    "# Creating a csv file for lat and lon with 3 digits \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"predictions_for_testing/new_lat_lon_model_with_3digits.csv\", index=False, float_format=\"%.8f\")\n",
    "\n",
    "print(f\"Total image paths processed: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4087ebe-9485-4a09-b772-e9977fefa09b",
   "metadata": {},
   "source": [
    "# Loading and Predicting FFNNLatLonModel Without 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a261b9ac-edfe-4b96-b861-f32913f8d88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in the dataset: 2112\n",
      "Total image paths processed: 212\n"
     ]
    }
   ],
   "source": [
    "lat_lon_model_without_3digits_path = \"ffnn_location_shortened_digits(3).pt\"\n",
    "\n",
    "lat_lon_model_without_3digits = LatLonModelFFNNWithOut3Digits()\n",
    "lat_lon_model_without_3digits.load_state_dict(torch.load(lat_lon_model_without_3digits_path, weights_only=True))  # Load the saved weights\n",
    "lat_lon_model_without_3digits.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Making predictions and saving them\n",
    "annotation_file = 'yolo_annotations.json'\n",
    "found_image_dir = \"test_7/captured_images_test7\"\n",
    "lat_lon_dataset_without_3digits = YOLODataset(annotation_file, found_image_dir)\n",
    "lat_lon_test_dataloader = DataLoader(lat_lon_dataset_without_3digits, batch_size=10, shuffle=False)\n",
    "\n",
    "print(f\"Total images in the dataset: {len(lat_lon_dataset_without_3digits)}\")\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "image_paths = []  # Uncommented to track image paths\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in lat_lon_test_dataloader:\n",
    "        # Predict outputs\n",
    "        outputs = lat_lon_model_without_3digits(inputs)\n",
    "        \n",
    "        all_predictions.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        image_paths.append(targets)\n",
    "\n",
    "flat_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()  # Converts to numpy array\n",
    "flat_image_paths = [path for batch in image_paths for path in batch]  # Flatten image paths if batched\n",
    "\n",
    "# Adjusting lat/lon as per the required formula\n",
    "adjusted_predictions_lat = []\n",
    "adjusted_predictions_lon = []\n",
    "\n",
    "\n",
    "for i in range(len(flat_predictions)):\n",
    "    predicted_lat, predicted_lon = flat_predictions[i]\n",
    "\n",
    "    predicted_lat = predicted_lat / 1000.0 + 35.7\n",
    "    predicted_lon = predicted_lon / -1000.0 - 78.6\n",
    "    \n",
    "    adjusted_predictions_lat.append(predicted_lat)\n",
    "    adjusted_predictions_lon.append(predicted_lon)\n",
    "\n",
    "# Creating a DataFrame for lat and lon with 3 digits \n",
    "data = {\n",
    "    \"image_path\": flat_image_paths,\n",
    "    \"predicted_lat\": adjusted_predictions_lat,\n",
    "    \"predicted_lon\": adjusted_predictions_lon,\n",
    "\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"predictions_for_testing/YOLO_FFNN_lat_lon_without_3digits(1).csv\", index=False, float_format=\"%.8f\")\n",
    "\n",
    "print(f\"Total image paths processed: {len(image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266dc52-a0b6-4174-8054-f741c5664dfe",
   "metadata": {},
   "source": [
    "# Loading and Predicting LSTMLatLonModel Without 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "523355ab-975f-4905-8210-fe95c06fa000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in the dataset: 2112\n",
      "Total image paths processed: 212\n"
     ]
    }
   ],
   "source": [
    "lstm_model_without_3digits_path = \"lstm_location_shortened_digits(3).pt\"\n",
    "\n",
    "lstm_model_without_3digits = LatLonModelLSTMWithout3digits()\n",
    "lstm_model_without_3digits.load_state_dict(torch.load(lstm_model_without_3digits_path, weights_only=True))\n",
    "lstm_model_without_3digits.eval()\n",
    "\n",
    "# Making predictions and saving them\n",
    "annotation_file = 'yolo_annotations.json'\n",
    "found_image_dir = \"test_7/captured_images_test7\"\n",
    "lat_lon_dataset_without_3digits = COCODataset(annotation_file, found_image_dir)\n",
    "lat_lon_test_dataloader = DataLoader(lat_lon_dataset_without_3digits, batch_size=10, shuffle=False)\n",
    "\n",
    "print(f\"Total images in the dataset: {len(lat_lon_dataset_without_3digits)}\")\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "image_paths = []  # Uncommented to track image paths\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in lat_lon_test_dataloader:\n",
    "        # Predict outputs\n",
    "        outputs = lstm_model_without_3digits(inputs)\n",
    "        \n",
    "        all_predictions.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        image_paths.append(targets)\n",
    "\n",
    "flat_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()  # Converts to numpy array\n",
    "flat_image_paths = [path for batch in image_paths for path in batch]  # Flatten image paths if batched\n",
    "\n",
    "# Adjusting lat/lon as per the required formula\n",
    "adjusted_predictions_lat = []\n",
    "adjusted_predictions_lon = []\n",
    "\n",
    "\n",
    "for i in range(len(flat_predictions)):\n",
    "    predicted_lat, predicted_lon = flat_predictions[i]\n",
    "\n",
    "    predicted_lat = predicted_lat / 1000.0 + 35.7\n",
    "    predicted_lon = predicted_lon / -1000.0 - 78.6\n",
    "    \n",
    "    adjusted_predictions_lat.append(predicted_lat)\n",
    "    adjusted_predictions_lon.append(predicted_lon)\n",
    "\n",
    "# Creating a DataFrame for lat and lon with 3 digits \n",
    "data = {\n",
    "    \"image_path\": flat_image_paths,\n",
    "    \"predicted_lat\": adjusted_predictions_lat,\n",
    "    \"predicted_lon\": adjusted_predictions_lon,\n",
    "\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"predictions_for_testing/YOLO_LSTM_lat_lon_without_3digits.csv\", index=False, float_format=\"%.8f\")\n",
    "\n",
    "print(f\"Total image paths processed: {len(image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39cc71b-a405-450d-8a0f-e6fcd6685757",
   "metadata": {},
   "source": [
    "# Loading and Predicting LSTMLatLonModel With 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43b71670-8207-4e48-a8a0-909e5467a4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in the dataset: 2112\n",
      "Total image paths processed: 212\n"
     ]
    }
   ],
   "source": [
    "lstm_model_without_3digits_path = \"lstm_location_no_shortened_digits.pt\"\n",
    "\n",
    "lstm_model_without_3digits = LatLonModelLSTMWithout3digits()\n",
    "lstm_model_without_3digits.load_state_dict(torch.load(lstm_model_without_3digits_path, weights_only=True))\n",
    "lstm_model_without_3digits.eval()\n",
    "\n",
    "# Making predictions and saving them\n",
    "annotation_file = 'yolo_annotations.json'\n",
    "found_image_dir = \"test_7/captured_images_test7\"\n",
    "lat_lon_dataset_without_3digits = COCODataset(annotation_file, found_image_dir)\n",
    "lat_lon_test_dataloader = DataLoader(lat_lon_dataset_without_3digits, batch_size=10, shuffle=False)\n",
    "\n",
    "print(f\"Total images in the dataset: {len(lat_lon_dataset_without_3digits)}\")\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "image_paths = []  # Uncommented to track image paths\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in lat_lon_test_dataloader:\n",
    "        # Predict outputs\n",
    "        outputs = lstm_model_without_3digits(inputs)\n",
    "        \n",
    "        all_predictions.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        image_paths.append(targets)\n",
    "\n",
    "flat_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()  # Converts to numpy array\n",
    "flat_image_paths = [path for batch in image_paths for path in batch]  # Flatten image paths if batched\n",
    "\n",
    "# Adjusting lat/lon as per the required formula\n",
    "adjusted_predictions_lat = []\n",
    "adjusted_predictions_lon = []\n",
    "\n",
    "\n",
    "for i in range(len(flat_predictions)):\n",
    "    predicted_lat, predicted_lon = flat_predictions[i]\n",
    "\n",
    "    predicted_lat = predicted_lat\n",
    "    predicted_lon = predicted_lon\n",
    "    \n",
    "    adjusted_predictions_lat.append(predicted_lat)\n",
    "    adjusted_predictions_lon.append(predicted_lon)\n",
    "\n",
    "# Creating a DataFrame for lat and lon with 3 digits \n",
    "data = {\n",
    "    \"image_path\": flat_image_paths,\n",
    "    \"predicted_lat\": adjusted_predictions_lat,\n",
    "    \"predicted_lon\": adjusted_predictions_lon,\n",
    "\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"predictions_for_testing/YOLO_LSTM_lat_lon_with_3digits.csv\", index=False, float_format=\"%.8f\")\n",
    "\n",
    "print(f\"Total image paths processed: {len(image_paths)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
