{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "24da1e8c-359c-476f-bbd9-c57068f9da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import patches\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn.functional as F  # Make sure this import is here\n",
    "import torch.nn as nn\n",
    "import shutil  # Used for copying files\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d847acd5-f513-44aa-a8a5-525676adb764",
   "metadata": {},
   "source": [
    "# Open the csv file, get all the image name and let the faster RCNN predict what the class it is and stuff\n",
    "# Create a coco dataset with all the images and bounding box\n",
    "# Use the images and bounding box to predict the lat lon for different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c438a660-84c7-453f-8161-a30dc6644ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is required for visualization of the images with bounding box\n",
    "# class COCODataset(Dataset):\n",
    "#     def __init__(self, annotation_file, image_dir, transforms=None):\n",
    "#         self.coco = COCO(annotation_file)\n",
    "#         self.image_dir = image_dir\n",
    "#         self.transforms = transforms\n",
    "#         self.ids = list(self.coco.imgs.keys())  # List of image IDs\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # Get the image ID and load the associated annotations\n",
    "#         img_id = self.ids[index]\n",
    "#         ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "#         anns = self.coco.loadAnns(ann_ids)\n",
    "#         image_info = self.coco.loadImgs(img_id)[0]\n",
    "#         path = image_info['file_name']\n",
    "\n",
    "#         # Load the image using OpenCV\n",
    "#         img = cv2.imread(os.path.join(self.image_dir, path))\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         num_objs = len(anns)\n",
    "#         boxes = []\n",
    "#         labels = []\n",
    "        \n",
    "#         # Extract the bounding boxes and category labels\n",
    "#         for i in range(num_objs):\n",
    "#             xmin = anns[i]['bbox'][0]\n",
    "#             ymin = anns[i]['bbox'][1]\n",
    "#             xmax = xmin + anns[i]['bbox'][2]\n",
    "#             ymax = ymin + anns[i]['bbox'][3]\n",
    "#             boxes.append([xmin, ymin, xmax, ymax])\n",
    "#             labels.append(anns[i]['category_id'])\n",
    "\n",
    "#         # Convert boxes and labels to tensors\n",
    "#         boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "#         if boxes.ndim == 1:\n",
    "#             boxes = boxes.unsqueeze(0)\n",
    "#         labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "#         image_id = torch.tensor([img_id])\n",
    "        \n",
    "#         # Calculate the area of the boxes\n",
    "#         if boxes.size(0) > 0:  # Check if there are any boxes\n",
    "#             area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "#         else:\n",
    "#             area = torch.tensor([])\n",
    "\n",
    "#         # Set the crowd flag to 0 (no crowd annotations in this case)\n",
    "#         iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "#         # Build the target dictionary\n",
    "#         target = {}\n",
    "#         target[\"boxes\"] = boxes\n",
    "#         target[\"labels\"] = labels\n",
    "#         target[\"image_id\"] = image_id\n",
    "#         target[\"area\"] = area\n",
    "#         target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "#         # Apply transformations if specified\n",
    "#         if self.transforms:\n",
    "#             img = self.transforms(img)\n",
    "\n",
    "#         return img, target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ids)\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, coco_file, images_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco_annotations = self.load_coco_annotations(coco_file)\n",
    "        self.bounding_boxes, self.image_files = self.process_data(self.coco_annotations)\n",
    "\n",
    "    def load_coco_annotations(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "        return coco_data\n",
    "\n",
    "    def process_data(self, coco_data):\n",
    "        max_instances_per_class = 2\n",
    "        num_classes = 9\n",
    "        data_points = 4\n",
    "        input_size = num_classes * max_instances_per_class * data_points\n",
    "        bounding_boxes = []\n",
    "        image_files = []\n",
    "\n",
    "        for image_info in coco_data['images']:\n",
    "            image_id = image_info['id']\n",
    "\n",
    "            input_vector = [0] * input_size\n",
    "            annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_id]\n",
    "            for ann in annotations:\n",
    "                class_id = ann['category_id']\n",
    "                bbox = ann['bbox']\n",
    "                instance_index = sum(1 for a in annotations if a['category_id'] == class_id) - 1\n",
    "                if instance_index < max_instances_per_class:\n",
    "                    start_index = (class_id * max_instances_per_class + instance_index) * 4\n",
    "                    length = 1280.0\n",
    "                    width = 720.0\n",
    "                    bbox[0] = bbox[0] / length\n",
    "                    bbox[1] = bbox[1] / width\n",
    "                    bbox[2] = bbox[2] / length\n",
    "                    bbox[3] = bbox[3] / width\n",
    "                    input_vector[start_index:start_index + 4] = bbox\n",
    "\n",
    "            bounding_boxes.append(input_vector)\n",
    "            image_files.append(image_info['file_name'])\n",
    "\n",
    "        return bounding_boxes, image_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bounding_boxes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bounding_box = torch.tensor(self.bounding_boxes[idx], dtype=torch.float32)\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_file)\n",
    "        return bounding_box, image_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5484890-5350-4f9e-b99d-417ea9ce1977",
   "metadata": {},
   "source": [
    "# Model (FFNN, LSTM) Architecture for Lat and Lon Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6091bc2-4052-4107-b740-01dd557abc10",
   "metadata": {},
   "source": [
    "# FFNN Architecture with first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf9a3896-ffbe-46c2-bb51-76dafbd7781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"ffnn_location_no_shortened_digits.pt\")\n",
    "# print(model)\n",
    "class LatLonModelWith3Digits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LatLonModelWith3Digits, self).__init__()\n",
    "        self.fc1 = nn.Linear(72, 40)\n",
    "        self.bn1 = nn.BatchNorm1d(40)\n",
    "        self.fc2 = nn.Linear(40, 2)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116a801-c685-4ea0-8da9-54e24c98a6f4",
   "metadata": {},
   "source": [
    "# FFNN Architecture without first 3 digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f42c1-d341-4d56-a9a5-6b12223e1afb",
   "metadata": {},
   "source": [
    "# LSTM Architecture with first 3 digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d1850-c325-49d8-bdfc-6e208de4c398",
   "metadata": {},
   "source": [
    "# LSTM Architecture without first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c7d588a9-b8a6-4de0-b095-e5dd402c9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to open the CSV file, correct the image names, and extract swift lat/lon data\n",
    "def open_csv(filepath):\n",
    "    \"\"\"Opens CSV, returns modified filenames and corresponding swift lat/lon.\"\"\"\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')  # Ensure correct encoding\n",
    "\n",
    "    corrected_image = []\n",
    "    swift_data = []\n",
    "\n",
    "    # Making the correct format and extract swift_lat, swift_lon\n",
    "    for index, row in df.iterrows():\n",
    "        image = row[\"timestamp\"]\n",
    "        if isinstance(image, str):  # Ensure the value is a string\n",
    "            corrected_img_name = image.replace(\":\", \"_\").strip()  # Replace \":\" and remove leading/trailing spaces\n",
    "            # Appending darker, darkest image\n",
    "            darker_img = corrected_img_name + \"_darker.jpg\"\n",
    "            darkest_img = corrected_img_name + \"_darkest.jpg\"\n",
    "            corrected_image.append((corrected_img_name + \".jpg\", row[\"swift_latitude\"], row[\"swift_longitude\"]))\n",
    "            corrected_image.append((darker_img, row[\"swift_latitude\"], row[\"swift_longitude\"]))\n",
    "            corrected_image.append((darkest_img, row[\"swift_latitude\"], row[\"swift_longitude\"]))\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return corrected_image\n",
    "\n",
    "csv_path = \"test_5/position_data_logger.csv\"\n",
    "corrected_image = open_csv(csv_path)\n",
    "# print(corrected_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a4150b75-4461-4ef9-8d07-31c709ccefc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating COCO-style annotations.\n"
     ]
    }
   ],
   "source": [
    "# Load the Faster RCNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one (for 9 classes + background)\n",
    "num_classes = 9 + 1  # 9 classes + background\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Load the model state dict (from previously saved model)\n",
    "model_save_path = \"faster_rcnn.pth\"\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Categories for COCO format\n",
    "categories = [{\"id\": i, \"name\": f\"class_{i}\"} for i in range(1, 10)]  # Modify with actual category names if you have them\n",
    "\n",
    "def load_image(file_path):\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    transform = ToTensor()  # Create a ToTensor transformation\n",
    "    return transform(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "\n",
    "# Function to annotate images in COCO format\n",
    "def annotate_images_in_folder(found_images, folder_path, output_json_path, threshold=0.5):\n",
    "    image_id = 1\n",
    "    annotation_id = 1\n",
    "    coco_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    # print(found_images)\n",
    "    # Loop through found images, checking if they exist in the folder\n",
    "    for image_info in found_images:\n",
    "        filename, swift_latitude, swift_longitude = image_info \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            # print(f\"Image not found: {filename}\")\n",
    "            continue  # Skip if the image is not found\n",
    "\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Load and preprocess the image\n",
    "            image = load_image(file_path).to(device)\n",
    "\n",
    "            # Predict using Faster R-CNN\n",
    "            with torch.no_grad():\n",
    "                prediction = model(image)\n",
    "\n",
    "            # Get image dimensions\n",
    "            image_np = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "            height, width, _ = image_np.shape\n",
    "\n",
    "            # Append image details to COCO annotations\n",
    "            coco_annotations[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": filename,\n",
    "                \"width\": int(width),\n",
    "                \"height\": int(height),\n",
    "                \"swift_latitude\": float(swift_latitude),  # Add swift latitude\n",
    "                \"swift_longitude\": float(swift_longitude)  # Add swift longitude\n",
    "            })\n",
    "\n",
    "            # Get the predictions\n",
    "            boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "            scores = prediction[0]['scores'].cpu().numpy()\n",
    "            labels = prediction[0]['labels'].cpu().numpy()\n",
    "\n",
    "            # Create a list of (score, label, box) tuples and sort by score descending\n",
    "            predictions = sorted(zip(scores, labels, boxes), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Keep track of the count of predictions per label type (to limit to 2 per class)\n",
    "            label_counts = {}\n",
    "\n",
    "            # Add top 2 predictions for each class\n",
    "            for score, label, box in predictions:\n",
    "                if score >= threshold:\n",
    "                    if label not in label_counts:\n",
    "                        label_counts[label] = 0\n",
    "                    if label_counts[label] < 2:\n",
    "                        xmin, ymin, xmax, ymax = box\n",
    "                        width = xmax - xmin\n",
    "                        height = ymax - ymin\n",
    "                        coco_annotations[\"annotations\"].append({\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": int(label),\n",
    "                            \"bbox\": [float(xmin), float(ymin), float(width), float(height)],\n",
    "                            \"score\": float(score)\n",
    "                        })\n",
    "                        label_counts[label] += 1\n",
    "                        annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "\n",
    "    # Save the annotations to a JSON file\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(coco_annotations, f, indent=4)\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'test_5/captured_images'  # The folder where the images are stored\n",
    "output_json_path = 'test_5/New_2BBox_Each_Class_Annotation.json'\n",
    "\n",
    "# Annotate images and create a COCO-style dataset\n",
    "annotate_images_in_folder(corrected_image, folder_path, output_json_path)\n",
    "print(\"Finished creating COCO-style annotations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93854fa1-97a5-4ff7-958c-d5dff2ebc10b",
   "metadata": {},
   "source": [
    "# Checking to make sure the model is predicting the bbox right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d221e687-f7b8-488d-a0f6-bb0f83ec4496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get a batch of data\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Visualize the batch\u001b[39;00m\n\u001b[1;32m     27\u001b[0m visualize_batch(images, targets)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "annotation_file = 'test_5/New_2BBox_Each_Class_Annotation.json'\n",
    "image_dir = 'test_5_found_image'\n",
    "dataset = COCODataset(annotation_file, image_dir, transforms=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "# Function to visualize a batch of images\n",
    "def visualize_batch(images, targets):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(20, 5))\n",
    "    for i, (img, target) in enumerate(zip(images, targets)):\n",
    "        img = img.permute(1, 2, 0).numpy()  # Convert tensor to numpy array\n",
    "        img = (img * 255).astype(np.uint8)  # Convert to uint8\n",
    "        axes[i].imshow(img)\n",
    "        boxes = target['boxes'].numpy()\n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red')\n",
    "            axes[i].add_patch(rect)\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of data\n",
    "images, targets = next(iter(dataloader))\n",
    "\n",
    "# Visualize the batch\n",
    "visualize_batch(images, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6dfe3-0dff-47f3-8906-42ca7651f93a",
   "metadata": {},
   "source": [
    "# Loading LatLonModelWith3Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ef7427af-37db-4f9a-a69d-a0bd2d086f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in the dataset: 833\n",
      "Number of batches processed: 84\n",
      "Number of predictions made: 833\n",
      "Total image paths processed: 84\n"
     ]
    }
   ],
   "source": [
    "lat_lon_model_with_3digits = LatLonModelWith3Digits()\n",
    "lat_lon_model_with_3digits.load_state_dict(torch.load('ffnn_location_no_shortened_digits.pt', weights_only=True))  # Load the saved weights\n",
    "lat_lon_model_with_3digits.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Making predictions and saving them\n",
    "annotation_file = 'test_5/New_2BBox_Each_Class_Annotation.json'\n",
    "found_image_dir = \"test_5_found_image\"\n",
    "lat_lon_dataset_with_3digits = COCODataset(annotation_file, found_image_dir)\n",
    "lat_lon_test_dataloader = DataLoader(lat_lon_dataset_with_3digits, batch_size=10, shuffle=False)\n",
    "\n",
    "print(f\"Total images in the dataset: {len(lat_lon_dataset_with_3digits)}\")\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "image_paths = []  # Uncommented to track image paths\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in lat_lon_test_dataloader:\n",
    "        # Predict outputs\n",
    "        outputs = lat_lon_model_with_3digits(inputs)\n",
    "        \n",
    "        \n",
    "        all_predictions.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        \n",
    "        image_paths.append(targets)  \n",
    "\n",
    "flat_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()  # Converts to numpy array\n",
    "flat_image_paths = [path for batch in image_paths for path in batch]  # Flatten image paths if batched\n",
    "\n",
    "# Check if all images were processed\n",
    "print(f\"Number of batches processed: {len(all_predictions)}\")\n",
    "print(f\"Number of predictions made: {sum([len(batch) for batch in all_predictions])}\")\n",
    "# print(all_targets)\n",
    "# Optionally, print or check specific outputs\n",
    "# for i, prediction in enumerate(all_predictions):\n",
    "#     print(f\"Batch {i}: {prediction}\")\n",
    "\n",
    "data = {\n",
    "    \"image_path\" : flat_image_paths,\n",
    "    \"predicted_lat\" : flat_predictions[:, 0],\n",
    "    \"predicted_lon\": flat_predictions[:, 1]\n",
    "}\n",
    "\n",
    "# Creating a csv file for lat and lon with 3 digits \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"predictions_for_testing/new_lat_lon_model_with_3digits.csv\", index=False)\n",
    "\n",
    "print(f\"Total image paths processed: {len(image_paths)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
