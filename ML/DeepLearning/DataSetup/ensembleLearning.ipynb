{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235971a6-d842-4ea3-a8bd-ee83f2a9623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea385f-e510-496a-935a-39ff9bd201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "rf = Roboflow(api_key=api_key)\n",
    "project = rf.workspace().project(\"university-landmark-detection\")\n",
    "model = project.version(3).model\n",
    "\n",
    "image_path = \"/resized_dataset/Train/09_48_43.jpg\"\n",
    "# infer on a local image\n",
    "print(model.predict(image_path, confidence=40, overlap=30).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8994429-7188-4edb-b746-a77e704b5eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Error during prediction for Test_2/captured_images_test2/396_darker.jpg: HTTPSConnectionPool(host='detect.roboflow.com', port=443): Max retries exceeded with url: /university-landmark-detection/3?api_key=uucie5AkN9CBB31xOHbp&name=YOUR_IMAGE.jpg&overlap=30&confidence=40&stroke=1&labels=false&format=json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x747aceaac160>: Failed to resolve 'detect.roboflow.com' ([Errno -3] Temporary failure in name resolution)\"))\n",
      "COCO dataset saved to Test_2/annotations_coco.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from roboflow import Roboflow\n",
    "import cv2\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "api_key = os.getenv('API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"API_KEY not found. Please ensure it's correctly set in the .env file.\")\n",
    "\n",
    "# Initialize Roboflow with the API key\n",
    "rf = Roboflow(api_key=api_key)\n",
    "\n",
    "# Access the specific project and model version\n",
    "project_name = \"university-landmark-detection\"\n",
    "version_number = 3\n",
    "\n",
    "try:\n",
    "    project = rf.workspace().project(project_name)\n",
    "    model = project.version(version_number).model\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error accessing project/version: {e}\")\n",
    "\n",
    "# Define the path to the folder containing images\n",
    "folder_path = \"Test_2/captured_images_test2\"\n",
    "output_coco_file = \"Test_2/annotations_coco.json\"\n",
    "\n",
    "# Create a COCO dataset structure\n",
    "coco_dataset = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": []\n",
    "}\n",
    "\n",
    "# Add categories based on the provided class names\n",
    "class_names = [\n",
    "    \"EB1\", \"EB2\", \"EB3\", \"FW\", \"HUNT\", \"OVAL\", \"building\", \"security-station\", \n",
    "    \"sign\", \"street-lamp\", \"structure\", \"trashcan\"\n",
    "]\n",
    "categories = [{\"id\": idx + 1, \"name\": name} for idx, name in enumerate(class_names)]\n",
    "coco_dataset[\"categories\"].extend(categories)\n",
    "\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "# Process each image in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Run prediction on the image\n",
    "        try:\n",
    "            prediction = model.predict(image_path, confidence=40, overlap=30).json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction for {image_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Load the image to get its dimensions\n",
    "        image = cv2.imread(image_path)\n",
    "        height, width, _ = image.shape\n",
    "        \n",
    "        # Add image info to COCO dataset\n",
    "        image_info = {\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": filename,\n",
    "            \"height\": height,\n",
    "            \"width\": width\n",
    "        }\n",
    "        coco_dataset[\"images\"].append(image_info)\n",
    "        \n",
    "        # Process predictions and add annotations to COCO dataset\n",
    "        for pred in prediction['predictions']:\n",
    "            x = pred['x'] - pred['width'] / 2\n",
    "            y = pred['y'] - pred['height'] / 2\n",
    "            width = pred['width']\n",
    "            height = pred['height']\n",
    "            confidence = pred['confidence']\n",
    "            class_name = pred['class']\n",
    "            \n",
    "            # Map class name to category ID\n",
    "            category_id = next((cat['id'] for cat in categories if cat['name'] == class_name), None)\n",
    "            if category_id is None:\n",
    "                print(f\"Warning: class '{class_name}' not found in categories.\")\n",
    "                continue\n",
    "            \n",
    "            annotation = {\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, width, height],\n",
    "                \"score\": confidence,\n",
    "                \"area\": width * height,\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            coco_dataset[\"annotations\"].append(annotation)\n",
    "            annotation_id += 1\n",
    "        \n",
    "        image_id += 1\n",
    "\n",
    "# Save the COCO dataset to a JSON file\n",
    "with open(output_coco_file, 'w') as f:\n",
    "    json.dump(coco_dataset, f, indent=4)\n",
    "\n",
    "print(f\"COCO dataset saved to {output_coco_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
