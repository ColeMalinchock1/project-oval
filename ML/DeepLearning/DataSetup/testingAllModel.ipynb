{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24da1e8c-359c-476f-bbd9-c57068f9da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import patches\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn.functional as F  \n",
    "import torch.nn as nn\n",
    "import shutil  # Used for copying files\n",
    "from torchvision.transforms import ToTensor\n",
    "from ultralytics import YOLO\n",
    "from torchvision.transforms import ToTensor, Resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d847acd5-f513-44aa-a8a5-525676adb764",
   "metadata": {},
   "source": [
    "# Open the csv file, get all the image name and let the faster RCNN predict what the class it is and stuff\n",
    "# Create a coco dataset with all the images and bounding box\n",
    "# Use the images and bounding box to predict the lat lon for different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c438a660-84c7-453f-8161-a30dc6644ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is required for visualization of the images with bounding box\n",
    "# class COCODataset(Dataset):\n",
    "#     def __init__(self, annotation_file, image_dir, transforms=None):\n",
    "#         self.coco = COCO(annotation_file)\n",
    "#         self.image_dir = image_dir\n",
    "#         self.transforms = transforms\n",
    "#         self.ids = list(self.coco.imgs.keys())  # List of image IDs\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # Get the image ID and load the associated annotations\n",
    "#         img_id = self.ids[index]\n",
    "#         ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "#         anns = self.coco.loadAnns(ann_ids)\n",
    "#         image_info = self.coco.loadImgs(img_id)[0]\n",
    "#         path = image_info['file_name']\n",
    "\n",
    "#         # Load the image using OpenCV\n",
    "#         img = cv2.imread(os.path.join(self.image_dir, path))\n",
    "        \n",
    "#         if img is None:\n",
    "#             raise FileNotFoundError(f\"Image not found at path: {img_path}\")\n",
    "\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         num_objs = len(anns)\n",
    "#         boxes = []\n",
    "#         labels = []\n",
    "        \n",
    "#         # Extract the bounding boxes and category labels\n",
    "#         for i in range(num_objs):\n",
    "#             xmin = anns[i]['bbox'][0]\n",
    "#             ymin = anns[i]['bbox'][1]\n",
    "#             xmax = xmin + anns[i]['bbox'][2]\n",
    "#             ymax = ymin + anns[i]['bbox'][3]\n",
    "#             boxes.append([xmin, ymin, xmax, ymax])\n",
    "#             labels.append(anns[i]['category_id'])\n",
    "\n",
    "#         # Convert boxes and labels to tensors\n",
    "#         boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "#         if boxes.ndim == 1:\n",
    "#             boxes = boxes.unsqueeze(0)\n",
    "#         labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "#         image_id = torch.tensor([img_id])\n",
    "        \n",
    "#         # Calculate the area of the boxes\n",
    "#         if boxes.size(0) > 0:  # Check if there are any boxes\n",
    "#             area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "#         else:\n",
    "#             area = torch.tensor([])\n",
    "\n",
    "#         # Set the crowd flag to 0 (no crowd annotations in this case)\n",
    "#         iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "#         # Build the target dictionary\n",
    "#         target = {}\n",
    "#         target[\"boxes\"] = boxes\n",
    "#         target[\"labels\"] = labels\n",
    "#         target[\"image_id\"] = image_id\n",
    "#         target[\"area\"] = area\n",
    "#         target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "#         # Apply transformations if specified\n",
    "#         if self.transforms:\n",
    "#             img = self.transforms(img)\n",
    "\n",
    "#         return img, target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ids)\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, coco_file, images_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco_annotations = self.load_coco_annotations(coco_file)\n",
    "        self.bounding_boxes, self.image_files = self.process_data(self.coco_annotations)\n",
    "\n",
    "    def load_coco_annotations(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "        return coco_data\n",
    "\n",
    "    def process_data(self, coco_data):\n",
    "        max_instances_per_class = 2\n",
    "        num_classes = 9\n",
    "        data_points = 4\n",
    "        input_size = num_classes * max_instances_per_class * data_points\n",
    "        bounding_boxes = []\n",
    "        image_files = []\n",
    "\n",
    "        for image_info in coco_data['images']:\n",
    "            image_id = image_info['id']\n",
    "\n",
    "            input_vector = [0] * input_size\n",
    "            annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_id]\n",
    "            for ann in annotations:\n",
    "                class_id = ann['category_id']\n",
    "                bbox = ann['bbox']\n",
    "                instance_index = sum(1 for a in annotations if a['category_id'] == class_id) - 1\n",
    "                if instance_index < max_instances_per_class:\n",
    "                    start_index = (class_id * max_instances_per_class + instance_index) * 4\n",
    "                    length = 1280.0\n",
    "                    width = 720.0\n",
    "                    bbox[0] = bbox[0] / length\n",
    "                    bbox[1] = bbox[1] / width\n",
    "                    bbox[2] = bbox[2] / length\n",
    "                    bbox[3] = bbox[3] / width\n",
    "                    input_vector[start_index:start_index + 4] = bbox\n",
    "\n",
    "            bounding_boxes.append(input_vector)\n",
    "            image_files.append(image_info['file_name'])\n",
    "\n",
    "        return bounding_boxes, image_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bounding_boxes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bounding_box = torch.tensor(self.bounding_boxes[idx], dtype=torch.float32)\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_file)\n",
    "        return bounding_box, image_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5484890-5350-4f9e-b99d-417ea9ce1977",
   "metadata": {},
   "source": [
    "# Model (FFNN, LSTM) Architecture for Lat and Lon Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6091bc2-4052-4107-b740-01dd557abc10",
   "metadata": {},
   "source": [
    "# FFNN Architecture with first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9a3896-ffbe-46c2-bb51-76dafbd7781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatLonModelFFNNWith3Digits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LatLonModelWith3Digits, self).__init__()\n",
    "        self.fc1 = nn.Linear(72, 40)\n",
    "        self.bn1 = nn.BatchNorm1d(40)\n",
    "        self.fc2 = nn.Linear(40, 2)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116a801-c685-4ea0-8da9-54e24c98a6f4",
   "metadata": {},
   "source": [
    "# FFNN Architecture without first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e327b-9126-4113-9426-195be0c6a32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f5f42c1-d341-4d56-a9a5-6b12223e1afb",
   "metadata": {},
   "source": [
    "# LSTM Architecture with first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b755e7-1550-433c-9728-aee6b69b0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatLonModelLSTMWith3digits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LatLonModel, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=72, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        # Fully connected layers with consistent sizes\n",
    "        self.fc1 = nn.Linear(256, 256)  # Keep the output size same as input for skip connection\n",
    "        self.fc2 = nn.Linear(256, 128)  # Reduce size in the second layer\n",
    "        self.fc3 = nn.Linear(128, 2)    # Final layer for output\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]  # Take the output of the last time step\n",
    "        \n",
    "        # First fully connected layer with skip connection\n",
    "        residual = x  # Save input for the skip connection\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x += residual  # Add skip connection\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second fully connected layer (no skip connection here)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d1850-c325-49d8-bdfc-6e208de4c398",
   "metadata": {},
   "source": [
    "# LSTM Architecture without first 3 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28dfa6-514e-4e3c-bd5d-eb2fb1377f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatLonModelLSTMWithout3digits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LatLonModel, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=72, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        # Fully connected layers with consistent sizes\n",
    "        self.fc1 = nn.Linear(256, 256)  # Keep the output size same as input for skip connection\n",
    "        self.fc2 = nn.Linear(256, 128)  # Reduce size in the second layer\n",
    "        self.fc3 = nn.Linear(128, 2)    # Final layer for output\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension for LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]  # Take the output of the last time step\n",
    "        \n",
    "        # First fully connected layer with skip connection\n",
    "        residual = x  # Save input for the skip connection\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x += residual  # Add skip connection\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second fully connected layer (no skip connection here)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79268aaf-14d9-4777-a134-314a4e168112",
   "metadata": {},
   "source": [
    "# Get hold of data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d588a9-b8a6-4de0-b095-e5dd402c9ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to open the CSV file, correct the image names, and extract swift lat/lon data\n",
    "def open_csv(filepath):\n",
    "    \"\"\"Opens CSV, returns modified filenames and corresponding swift lat/lon.\"\"\"\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')  # Ensure correct encoding\n",
    "\n",
    "    corrected_image = []\n",
    "    swift_data = []\n",
    "    \n",
    "    # Making the correct format and extract swift_lat, swift_lon\n",
    "    for index, row in df.iterrows():\n",
    "        image = row[\"img_name\"]\n",
    "        \n",
    "        if isinstance(image, str):  # Ensure the value is a string\n",
    "            corrected_img_name = image.replace(\":\", \"_\").strip()  # Replace \":\" and remove leading/trailing spaces\n",
    "            # Appending darker, darkest image\n",
    "            darkest_img = corrected_img_name.replace(\".jpg\", \"_darkest.jpg\")\n",
    "            darker_img = corrected_img_name.replace(\".jpg\", \"_darker.jpg\")\n",
    "            corrected_image.append((corrected_img_name, row[\"swift_latitude\"], row[\"swift_longitude\"]))\n",
    "            corrected_image.append((darker_img, row[\"swift_latitude\"], row[\"swift_longitude\"]))\n",
    "            corrected_image.append((darkest_img, row[\"swift_latitude\"], row[\"swift_longitude\"]))\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return corrected_image \n",
    "\n",
    "csv_path = \"test_5/position_data_logger.csv\"\n",
    "corrected_image = open_csv(csv_path)\n",
    "# print(corrected_image)\n",
    "print(isinstance(corrected_image, list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283bf41-e441-43c1-8711-eded31d23cb0",
   "metadata": {},
   "source": [
    "# Creates a COCODataset using Faster RCNN for detecting 9 classes (sign, FW, HUNT, OVAL, engineering-building, security-station, sign, street-lamp, trashcan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4150b75-4461-4ef9-8d07-31c709ccefc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating COCO-style annotations.\n"
     ]
    }
   ],
   "source": [
    "# Load the Faster RCNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one (for 9 classes + background)\n",
    "num_classes = 9 + 1  # 9 classes + background\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Load the model state dict (from previously saved model)\n",
    "model_save_path = \"faster_rcnn.pth\"\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Categories for COCO format\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"sign\"},\n",
    "    {\"id\": 1, \"name\": \"FW\"},\n",
    "    {\"id\": 2, \"name\": \"HUNT\"},\n",
    "    {\"id\": 3, \"name\": \"OVAL\"},\n",
    "    {\"id\": 4, \"name\": \"engineering-building\"},\n",
    "    {\"id\": 5, \"name\": \"security-station\"},\n",
    "    {\"id\": 6, \"name\": \"sign\"},\n",
    "    {\"id\": 7, \"name\": \"street-lamp\"},\n",
    "    {\"id\": 8, \"name\": \"trashcan\"}\n",
    "]\n",
    "\n",
    "# ['sign', 'FW', 'HUNT', 'Oval', 'engineering-building', 'security-station', 'sign', 'street-lamp', 'trashcan']\n",
    "def load_image(file_path):\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    transform = ToTensor()  # Create a ToTensor transformation\n",
    "    return transform(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "\n",
    "# Function to annotate images in COCO format\n",
    "def annotate_images_in_folder(found_images, folder_path, output_json_path, threshold=0.5):\n",
    "    image_id = 1\n",
    "    annotation_id = 1\n",
    "    coco_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    # print(found_images)\n",
    "    # Loop through found images, checking if they exist in the folder\n",
    "    for image_info in found_images:\n",
    "        filename, swift_latitude, swift_longitude = image_info\n",
    "        # print(filename)\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            # print(f\"Image not found: {filename}\")\n",
    "            continue  # Skip if the image is not found\n",
    "\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Load and preprocess the image\n",
    "            image = load_image(file_path).to(device)\n",
    "\n",
    "            # Predict using Faster R-CNN\n",
    "            with torch.no_grad():\n",
    "                prediction = model(image)\n",
    "\n",
    "            # Get image dimensions\n",
    "            image_np = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "            height, width, _ = image_np.shape\n",
    "\n",
    "            # Append image details to COCO annotations\n",
    "            coco_annotations[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": filename,\n",
    "                \"width\": int(width),\n",
    "                \"height\": int(height),\n",
    "                \"swift_latitude\": float(swift_latitude),  # Add swift latitude\n",
    "                \"swift_longitude\": float(swift_longitude)  # Add swift longitude\n",
    "            })\n",
    "\n",
    "            # Get the predictions\n",
    "            boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "            scores = prediction[0]['scores'].cpu().numpy()\n",
    "            labels = prediction[0]['labels'].cpu().numpy()\n",
    "\n",
    "            # Create a list of (score, label, box) tuples and sort by score descending\n",
    "            predictions = sorted(zip(scores, labels, boxes), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Keep track of the count of predictions per label type (to limit to 2 per class)\n",
    "            label_counts = {}\n",
    "\n",
    "            # Add top 2 predictions for each class\n",
    "            for score, label, box in predictions:\n",
    "                if score >= threshold:\n",
    "                    if label not in label_counts:\n",
    "                        label_counts[label] = 0\n",
    "                    if label_counts[label] < 2:\n",
    "                        xmin, ymin, xmax, ymax = box\n",
    "                        width = xmax - xmin\n",
    "                        height = ymax - ymin\n",
    "                        coco_annotations[\"annotations\"].append({\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": int(label),\n",
    "                            \"bbox\": [float(xmin), float(ymin), float(width), float(height)],\n",
    "                            \"score\": float(score)\n",
    "                        })\n",
    "                        label_counts[label] += 1\n",
    "                        annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "\n",
    "    # Save the annotations to a JSON file\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(coco_annotations, f, indent=4)\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'test_5/captured_images'  # The folder where the images are stored\n",
    "output_json_path = 'test_5/2BBox_Each_Class_Annotation.json'\n",
    "\n",
    "# Annotate images and create a COCO-style dataset\n",
    "annotate_images_in_folder(corrected_image, folder_path, output_json_path)\n",
    "print(\"Finished creating COCO-style annotations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02995e-1f2f-4859-9ac8-2c2af7cdd234",
   "metadata": {},
   "source": [
    "# Creates a COCODataset using Yolo Model for detecting 9 classes ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b1b9b8-b987-4eec-8b3f-90325f8c9147",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOLO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m(model_path)\n\u001b[1;32m      3\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_image1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Warming up the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YOLO' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = \"best.pt\"\n",
    "yolo_model = YOLO(model_path)\n",
    "test_image = \"test_image1.jpg\"\n",
    "# Warming up the model\n",
    "_ = yolo_model(test_image)\n",
    "CUSTOM_CLASS_NAMES = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "def plot_detections(image_path, results):\n",
    "    # Load the image using OpenCV but use Matplotlib for display\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for Matplotlib\n",
    "    \n",
    "    # Create a Matplotlib figure and axis\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image_rgb)\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy\n",
    "        confidences = result.boxes.conf\n",
    "        class_ids = result.boxes.cls\n",
    "\n",
    "        for box, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "            x_min, y_min, x_max, y_max = map(int, box)\n",
    "            label = CUSTOM_CLASS_NAMES[int(class_id)]\n",
    "            score = float(confidence)\n",
    "\n",
    "            # Create a Rectangle patch for bounding box\n",
    "            rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='green', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add label and score\n",
    "            label_text = f'{label}: {score:.2f}'\n",
    "            ax.text(x_min, y_min - 5, label_text, color='white', fontsize=12, bbox=dict(facecolor='green', alpha=0.5))\n",
    "\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "result = yolo_model(test_image)\n",
    "plot_detections(test_image, result)\n",
    "\n",
    "# Categories for COCO format\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"sign\"},\n",
    "    {\"id\": 1, \"name\": \"FW\"},\n",
    "    {\"id\": 2, \"name\": \"HUNT\"},\n",
    "    {\"id\": 3, \"name\": \"OVAL\"},\n",
    "    {\"id\": 4, \"name\": \"engineering-building\"},\n",
    "    {\"id\": 5, \"name\": \"security-station\"},\n",
    "    {\"id\": 6, \"name\": \"sign\"},\n",
    "    {\"id\": 7, \"name\": \"street-lamp\"},\n",
    "    {\"id\": 8, \"name\": \"trashcan\"}\n",
    "]\n",
    "\n",
    "# Function to load and resize image\n",
    "def load_image(file_path):\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    resize_transform = Resize((640, 640))  # Resize image to 640x640\n",
    "    transform = ToTensor()  # Create a ToTensor transformation\n",
    "    image = resize_transform(image)  # Resize the image\n",
    "    return transform(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "\n",
    "# Function to annotate images in COCO format\n",
    "def annotate_images_in_folder(found_images, folder_path, output_json_path, threshold=0.5):\n",
    "    image_id = 1\n",
    "    annotation_id = 1\n",
    "    coco_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories\n",
    "    }\n",
    "\n",
    "    # Loop through found images, checking if they exist in the folder\n",
    "    for image_info in found_images:\n",
    "        filename, swift_latitude, swift_longitude = image_info\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            continue  # Skip if the image is not found\n",
    "\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Load and preprocess the image\n",
    "            image = load_image(file_path).to(device)\n",
    "\n",
    "            # Predict using YOLO model\n",
    "            with torch.no_grad():\n",
    "                prediction = yolo_model(image)[0]  # Adjusting here to get the first prediction result\n",
    "\n",
    "            # Get image dimensions\n",
    "            image_np = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "            height, width, _ = image_np.shape\n",
    "\n",
    "            # Append image details to COCO annotations\n",
    "            coco_annotations[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": filename,\n",
    "                \"width\": int(width),\n",
    "                \"height\": int(height),\n",
    "                \"swift_latitude\": float(swift_latitude),  # Add swift latitude\n",
    "                \"swift_longitude\": float(swift_longitude)  # Add swift longitude\n",
    "            })\n",
    "\n",
    "            # Extract predictions\n",
    "            boxes = prediction.boxes.xyxy.cpu().numpy()  # Bounding boxes\n",
    "            scores = prediction.boxes.conf.cpu().numpy()  # Confidence scores\n",
    "            labels = prediction.boxes.cls.cpu().numpy()  # Class labels\n",
    "\n",
    "            # Create a list of (score, label, box) tuples and sort by score descending\n",
    "            predictions = sorted(zip(scores, labels, boxes), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Keep track of the count of predictions per label type (to limit to 2 per class)\n",
    "            label_counts = {}\n",
    "\n",
    "            # Add top 2 predictions for each class\n",
    "            for score, label, box in predictions:\n",
    "                if score >= threshold:\n",
    "                    if label not in label_counts:\n",
    "                        label_counts[label] = 0\n",
    "                    if label_counts[label] < 2:\n",
    "                        xmin, ymin, xmax, ymax = box\n",
    "                        width = xmax - xmin\n",
    "                        height = ymax - ymin\n",
    "                        coco_annotations[\"annotations\"].append({\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": int(label),\n",
    "                            \"bbox\": [float(xmin), float(ymin), float(width), float(height)],\n",
    "                            \"score\": float(score)\n",
    "                        })\n",
    "                        label_counts[label] += 1\n",
    "                        annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "\n",
    "    # Save the annotations to a JSON file\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(coco_annotations, f, indent=4)\n",
    "\n",
    "# Example usage:\n",
    "folder_path = 'test_5/captured_images'  # The folder where the images are stored\n",
    "output_json_path = 'test_5/YOLO_2BBox_Each_Class_Annotation.json'\n",
    "\n",
    "# Annotate images and create a COCO-style dataset\n",
    "annotate_images_in_folder(corrected_image, folder_path, output_json_path)\n",
    "print(\"Finished creating COCO-style annotations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93854fa1-97a5-4ff7-958c-d5dff2ebc10b",
   "metadata": {},
   "source": [
    "# Checking to make sure the model is predicting the bbox right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d221e687-f7b8-488d-a0f6-bb0f83ec4496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@19.199] global loadsave.cpp:241 findDecoder imread_('test_5_found_image/2024-07-11-09_31_13.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@19.221] global loadsave.cpp:241 findDecoder imread_('test_5_found_image/2024-07-11-10_14_51.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@19.225] global loadsave.cpp:241 findDecoder imread_('test_5_found_image/2024-07-11-09_54_31.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pthapa4/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/pthapa4/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/pthapa4/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_369481/445493035.py\", line 47, in __getitem__\n    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\nIndexError: index 3 is out of bounds for dimension 1 with size 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get a batch of data\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Visualize the batch\u001b[39;00m\n\u001b[1;32m     27\u001b[0m visualize_batch(images, targets)\n",
      "File \u001b[0;32m~/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/pthapa4/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/pthapa4/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/pthapa4/Downloads/yes/envs/project-oval/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_369481/445493035.py\", line 47, in __getitem__\n    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\nIndexError: index 3 is out of bounds for dimension 1 with size 0\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "annotation_file = 'test_5/YOLO_2BBox_Each_Class_Annotation.json'\n",
    "image_dir = 'test_5_found_image'\n",
    "dataset = COCODataset(annotation_file, image_dir, transforms=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "# Function to visualize a batch of images\n",
    "def visualize_batch(images, targets):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(20, 5))\n",
    "    for i, (img, target) in enumerate(zip(images, targets)):\n",
    "        img = img.permute(1, 2, 0).numpy()  # Convert tensor to numpy array\n",
    "        img = (img * 255).astype(np.uint8)  # Convert to uint8\n",
    "        axes[i].imshow(img)\n",
    "        boxes = target['boxes'].numpy()\n",
    "        for box in boxes:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red')\n",
    "            axes[i].add_patch(rect)\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of data\n",
    "images, targets = next(iter(dataloader))\n",
    "\n",
    "# Visualize the batch\n",
    "visualize_batch(images, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6dfe3-0dff-47f3-8906-42ca7651f93a",
   "metadata": {},
   "source": [
    "# Loading LatLonModelWith3Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ef7427af-37db-4f9a-a69d-a0bd2d086f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in the dataset: 833\n",
      "[tensor([[ 35.77074432, -78.67474365],\n",
      "        [ 35.77074814, -78.67473602],\n",
      "        [ 35.77061462, -78.67485046],\n",
      "        [ 35.77061462, -78.67485046],\n",
      "        [ 35.77060318, -78.67469788],\n",
      "        [ 35.77045441, -78.67483521],\n",
      "        [ 35.77024460, -78.67494202],\n",
      "        [ 35.77035904, -78.67494202],\n",
      "        [ 35.77040863, -78.67486572],\n",
      "        [ 35.77042007, -78.67484283]]), tensor([[ 35.77040863, -78.67487335],\n",
      "        [ 35.77040863, -78.67487335],\n",
      "        [ 35.77040482, -78.67484283],\n",
      "        [ 35.77041626, -78.67483521],\n",
      "        [ 35.77053070, -78.67481232],\n",
      "        [ 35.77052307, -78.67481232],\n",
      "        [ 35.77044678, -78.67482758],\n",
      "        [ 35.77045441, -78.67481995],\n",
      "        [ 35.77044296, -78.67482758],\n",
      "        [ 35.77044678, -78.67482758]]), tensor([[ 35.77046585, -78.67481995],\n",
      "        [ 35.77046204, -78.67481995],\n",
      "        [ 35.77045441, -78.67482758],\n",
      "        [ 35.77064514, -78.67475891],\n",
      "        [ 35.77049637, -78.67493439],\n",
      "        [ 35.77031708, -78.67517853],\n",
      "        [ 35.77031708, -78.67517853],\n",
      "        [ 35.77028656, -78.67521667],\n",
      "        [ 35.77027512, -78.67522430],\n",
      "        [ 35.77021027, -78.67498779]]), tensor([[ 35.77023315, -78.67499542],\n",
      "        [ 35.77025604, -78.67525482],\n",
      "        [ 35.76996231, -78.67575836],\n",
      "        [ 35.76889801, -78.67759705],\n",
      "        [ 35.76906967, -78.67713165],\n",
      "        [ 35.76922226, -78.67691803],\n",
      "        [ 35.77027893, -78.67641449],\n",
      "        [ 35.76974869, -78.67609406],\n",
      "        [ 35.77016068, -78.67536163],\n",
      "        [ 35.77059937, -78.67483521]]), tensor([[ 35.77046967, -78.67517090],\n",
      "        [ 35.77047348, -78.67517090],\n",
      "        [ 35.77040863, -78.67514801],\n",
      "        [ 35.77010727, -78.67543793],\n",
      "        [ 35.77010727, -78.67543793],\n",
      "        [ 35.77041626, -78.67513275],\n",
      "        [ 35.77054977, -78.67507935],\n",
      "        [ 35.77055740, -78.67507172],\n",
      "        [ 35.77042389, -78.67512512],\n",
      "        [ 35.77067184, -78.67498779]]), tensor([[ 35.77059174, -78.67501068],\n",
      "        [ 35.77066040, -78.67491913],\n",
      "        [ 35.77056885, -78.67505646],\n",
      "        [ 35.77065277, -78.67493439],\n",
      "        [ 35.77081680, -78.67477417],\n",
      "        [ 35.77096176, -78.67465973],\n",
      "        [ 35.77075577, -78.67478180],\n",
      "        [ 35.77050781, -78.67520142],\n",
      "        [ 35.77060318, -78.67507172],\n",
      "        [ 35.77059937, -78.67507935]]), tensor([[ 35.77089310, -78.67463684],\n",
      "        [ 35.77088928, -78.67464447],\n",
      "        [ 35.77092743, -78.67457581],\n",
      "        [ 35.77087021, -78.67461395],\n",
      "        [ 35.77087402, -78.67461395],\n",
      "        [ 35.77051544, -78.67491913],\n",
      "        [ 35.77101135, -78.67445374],\n",
      "        [ 35.77083588, -78.67465210],\n",
      "        [ 35.77082062, -78.67467499],\n",
      "        [ 35.77072144, -78.67469788]]), tensor([[ 35.77107620, -78.67437744],\n",
      "        [ 35.77105331, -78.67438507],\n",
      "        [ 35.77086639, -78.67469025],\n",
      "        [ 35.77091599, -78.67462158],\n",
      "        [ 35.77090836, -78.67463684],\n",
      "        [ 35.77092743, -78.67460632],\n",
      "        [ 35.77115631, -78.67429352],\n",
      "        [ 35.77097321, -78.67446136],\n",
      "        [ 35.77107620, -78.67433167],\n",
      "        [ 35.77107620, -78.67432404]]), tensor([[ 35.77107620, -78.67432404],\n",
      "        [ 35.77114487, -78.67424774],\n",
      "        [ 35.77114105, -78.67425537],\n",
      "        [ 35.77117538, -78.67427063],\n",
      "        [ 35.77111435, -78.67427826],\n",
      "        [ 35.77112961, -78.67426300],\n",
      "        [ 35.77098083, -78.67443848],\n",
      "        [ 35.77108383, -78.67430878],\n",
      "        [ 35.77114487, -78.67424774],\n",
      "        [ 35.77112579, -78.67426300]]), tensor([[ 35.77112961, -78.67426300],\n",
      "        [ 35.77113724, -78.67425537],\n",
      "        [ 35.77112579, -78.67427063],\n",
      "        [ 35.77111816, -78.67427063],\n",
      "        [ 35.77111435, -78.67427826],\n",
      "        [ 35.77112198, -78.67427063],\n",
      "        [ 35.77109528, -78.67430115],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77090454, -78.67451477]]), tensor([[ 35.77090454, -78.67451477],\n",
      "        [ 35.77076340, -78.67465210],\n",
      "        [ 35.77088547, -78.67453003],\n",
      "        [ 35.77090073, -78.67447662],\n",
      "        [ 35.77041626, -78.67492676],\n",
      "        [ 35.77050018, -78.67477417],\n",
      "        [ 35.77050400, -78.67476654],\n",
      "        [ 35.77062607, -78.67473602],\n",
      "        [ 35.77035904, -78.67495728],\n",
      "        [ 35.77035904, -78.67494965]]), tensor([[ 35.77042770, -78.67485809],\n",
      "        [ 35.77037048, -78.67491913],\n",
      "        [ 35.77035904, -78.67496490],\n",
      "        [ 35.77038956, -78.67488861],\n",
      "        [ 35.77040100, -78.67487335],\n",
      "        [ 35.77043152, -78.67481232],\n",
      "        [ 35.77043152, -78.67481995],\n",
      "        [ 35.77035141, -78.67493439],\n",
      "        [ 35.77042007, -78.67484283],\n",
      "        [ 35.77042770, -78.67483521]]), tensor([[ 35.77037048, -78.67491150],\n",
      "        [ 35.77039719, -78.67488861],\n",
      "        [ 35.77043152, -78.67484283],\n",
      "        [ 35.77039337, -78.67492676],\n",
      "        [ 35.77038956, -78.67492676],\n",
      "        [ 35.77048874, -78.67482758],\n",
      "        [ 35.77048874, -78.67485046],\n",
      "        [ 35.77048874, -78.67485046],\n",
      "        [ 35.77047729, -78.67486572],\n",
      "        [ 35.77029037, -78.67514801]]), tensor([[ 35.77056503, -78.67485809],\n",
      "        [ 35.77045059, -78.67499542],\n",
      "        [ 35.77028656, -78.67520905],\n",
      "        [ 35.77031708, -78.67517090],\n",
      "        [ 35.77032471, -78.67516327],\n",
      "        [ 35.77023697, -78.67533112],\n",
      "        [ 35.77027512, -78.67522430],\n",
      "        [ 35.77021027, -78.67530823],\n",
      "        [ 35.77021027, -78.67530823],\n",
      "        [ 35.77020264, -78.67531586]]), tensor([[ 35.76994324, -78.67583466],\n",
      "        [ 35.77011871, -78.67543030],\n",
      "        [ 35.77009583, -78.67545319],\n",
      "        [ 35.77009964, -78.67544556],\n",
      "        [ 35.77009964, -78.67545319],\n",
      "        [ 35.76762009, -78.67855835],\n",
      "        [ 35.76785278, -78.67826843],\n",
      "        [ 35.77004623, -78.67568207],\n",
      "        [ 35.77001190, -78.67570496],\n",
      "        [ 35.76801682, -78.67768097]]), tensor([[ 35.76808167, -78.67758942],\n",
      "        [ 35.77021408, -78.67537689],\n",
      "        [ 35.77045059, -78.67520142],\n",
      "        [ 35.76999664, -78.67549133],\n",
      "        [ 35.77057266, -78.67504883],\n",
      "        [ 35.77062988, -78.67496490],\n",
      "        [ 35.77070236, -78.67494965],\n",
      "        [ 35.77077866, -78.67484283],\n",
      "        [ 35.77093506, -78.67469788],\n",
      "        [ 35.77095795, -78.67465973]]), tensor([[ 35.77084732, -78.67472839],\n",
      "        [ 35.77087402, -78.67493439],\n",
      "        [ 35.77073669, -78.67488098],\n",
      "        [ 35.77071381, -78.67491150],\n",
      "        [ 35.77054596, -78.67575836],\n",
      "        [ 35.77054596, -78.67578125],\n",
      "        [ 35.77088547, -78.67464447],\n",
      "        [ 35.77087402, -78.67465973],\n",
      "        [ 35.77094269, -78.67456818],\n",
      "        [ 35.77094650, -78.67456055]]), tensor([[ 35.77103043, -78.67443085],\n",
      "        [ 35.77102661, -78.67443848],\n",
      "        [ 35.77082443, -78.67466736],\n",
      "        [ 35.77082443, -78.67466736],\n",
      "        [ 35.77106094, -78.67439270],\n",
      "        [ 35.77104950, -78.67438507],\n",
      "        [ 35.77115250, -78.67436218],\n",
      "        [ 35.77118683, -78.67432404],\n",
      "        [ 35.77114105, -78.67434692],\n",
      "        [ 35.77118683, -78.67430878]]), tensor([[ 35.77117920, -78.67431641],\n",
      "        [ 35.77115631, -78.67433929],\n",
      "        [ 35.77098465, -78.67441559],\n",
      "        [ 35.77109528, -78.67435455],\n",
      "        [ 35.77100372, -78.67444611],\n",
      "        [ 35.77106476, -78.67440796],\n",
      "        [ 35.77113724, -78.67431641],\n",
      "        [ 35.77113342, -78.67432404],\n",
      "        [ 35.77107620, -78.67442322],\n",
      "        [ 35.77111816, -78.67433929]]), tensor([[ 35.77118301, -78.67427063],\n",
      "        [ 35.77098083, -78.67446899],\n",
      "        [ 35.77116013, -78.67433929],\n",
      "        [ 35.77096176, -78.67448425],\n",
      "        [ 35.77095413, -78.67449188],\n",
      "        [ 35.77100754, -78.67446899],\n",
      "        [ 35.77106857, -78.67437744],\n",
      "        [ 35.77096558, -78.67446899],\n",
      "        [ 35.77098465, -78.67444611],\n",
      "        [ 35.77112961, -78.67427063]]), tensor([[ 35.77114105, -78.67430878],\n",
      "        [ 35.77111816, -78.67433167],\n",
      "        [ 35.77104187, -78.67436981],\n",
      "        [ 35.77106094, -78.67434692],\n",
      "        [ 35.77189255, -78.67371368],\n",
      "        [ 35.77076721, -78.67466736],\n",
      "        [ 35.77062607, -78.67475891],\n",
      "        [ 35.77062988, -78.67475891],\n",
      "        [ 35.77069473, -78.67470551],\n",
      "        [ 35.77058792, -78.67481232]]), tensor([[ 35.77062988, -78.67475128],\n",
      "        [ 35.77003860, -78.67501068],\n",
      "        [ 35.77031326, -78.67512512],\n",
      "        [ 35.76665878, -78.67938232],\n",
      "        [ 35.77041626, -78.67510986],\n",
      "        [ 35.77047729, -78.67503357],\n",
      "        [ 35.77047729, -78.67503357],\n",
      "        [ 35.77048492, -78.67502594],\n",
      "        [ 35.77045441, -78.67503357],\n",
      "        [ 35.77030182, -78.67517090]]), tensor([[ 35.77032089, -78.67514038],\n",
      "        [ 35.77027893, -78.67520905],\n",
      "        [ 35.77031708, -78.67521667],\n",
      "        [ 35.77032471, -78.67519379],\n",
      "        [ 35.77040863, -78.67508698],\n",
      "        [ 35.77040482, -78.67508698],\n",
      "        [ 35.77038193, -78.67511749],\n",
      "        [ 35.77031326, -78.67517853],\n",
      "        [ 35.77022171, -78.67527771],\n",
      "        [ 35.77026749, -78.67522430]]), tensor([[ 35.76941681, -78.67646027],\n",
      "        [ 35.77040100, -78.67512512],\n",
      "        [ 35.77025604, -78.67518616],\n",
      "        [ 35.77025604, -78.67518616],\n",
      "        [ 35.77026367, -78.67518616],\n",
      "        [ 35.77035141, -78.67501068],\n",
      "        [ 35.77035522, -78.67501068],\n",
      "        [ 35.77066803, -78.67472076],\n",
      "        [ 35.77067184, -78.67472076],\n",
      "        [ 35.77039337, -78.67494965]]), tensor([[ 35.77028275, -78.67502594],\n",
      "        [ 35.77028656, -78.67502594],\n",
      "        [ 35.77032089, -78.67502594],\n",
      "        [ 35.77040863, -78.67491150],\n",
      "        [ 35.77041626, -78.67489624],\n",
      "        [ 35.77025604, -78.67513275],\n",
      "        [ 35.77040863, -78.67489624],\n",
      "        [ 35.77013397, -78.67521667],\n",
      "        [ 35.76888275, -78.67739868],\n",
      "        [ 35.76647949, -78.68002319]]), tensor([[ 35.77032089, -78.67496490],\n",
      "        [ 35.77040100, -78.67489624],\n",
      "        [ 35.77027512, -78.67504883],\n",
      "        [ 35.77033234, -78.67497253],\n",
      "        [ 35.77033234, -78.67497253],\n",
      "        [ 35.77032089, -78.67497253],\n",
      "        [ 35.77032089, -78.67497253],\n",
      "        [ 35.77032852, -78.67497253],\n",
      "        [ 35.77032852, -78.67497253],\n",
      "        [ 35.77033615, -78.67495728]]), tensor([[ 35.77072144, -78.67469788],\n",
      "        [ 35.77095032, -78.67451477],\n",
      "        [ 35.77093887, -78.67449951],\n",
      "        [ 35.77099991, -78.67444611],\n",
      "        [ 35.77097702, -78.67447662],\n",
      "        [ 35.77097702, -78.67448425],\n",
      "        [ 35.77087402, -78.67457581],\n",
      "        [ 35.77106094, -78.67443848],\n",
      "        [ 35.77172470, -78.67388916],\n",
      "        [ 35.77103806, -78.67445374]]), tensor([[ 35.77104568, -78.67444611],\n",
      "        [ 35.77121353, -78.67424011],\n",
      "        [ 35.77088928, -78.67455292],\n",
      "        [ 35.77095032, -78.67448425],\n",
      "        [ 35.77093124, -78.67450714],\n",
      "        [ 35.77117538, -78.67429352],\n",
      "        [ 35.77096939, -78.67449188],\n",
      "        [ 35.77134323, -78.67417908],\n",
      "        [ 35.77093887, -78.67449951],\n",
      "        [ 35.77108002, -78.67443085]]), tensor([[ 35.77257538, -78.67345428],\n",
      "        [ 35.77099991, -78.67451477],\n",
      "        [ 35.77111435, -78.67435455],\n",
      "        [ 35.77101517, -78.67443848],\n",
      "        [ 35.77101517, -78.67446899],\n",
      "        [ 35.77099228, -78.67450714],\n",
      "        [ 35.77111435, -78.67435455],\n",
      "        [ 35.77108002, -78.67439270],\n",
      "        [ 35.77103806, -78.67444611],\n",
      "        [ 35.77077866, -78.67472076]]), tensor([[ 35.77074814, -78.67468262],\n",
      "        [ 35.77079010, -78.67463684],\n",
      "        [ 35.77069855, -78.67470551],\n",
      "        [ 35.77069855, -78.67470551],\n",
      "        [ 35.77080536, -78.67462921],\n",
      "        [ 35.77061462, -78.67475128],\n",
      "        [ 35.77074814, -78.67465210],\n",
      "        [ 35.77074814, -78.67465210],\n",
      "        [ 35.77061844, -78.67475891],\n",
      "        [ 35.76947784, -78.67593384]]), tensor([[ 35.76946640, -78.67594147],\n",
      "        [ 35.77036285, -78.67517090],\n",
      "        [ 35.77039719, -78.67512512],\n",
      "        [ 35.77039719, -78.67512512],\n",
      "        [ 35.77039337, -78.67512512],\n",
      "        [ 35.77045822, -78.67491913],\n",
      "        [ 35.77043533, -78.67491913],\n",
      "        [ 35.77042770, -78.67492676],\n",
      "        [ 35.77044678, -78.67488861],\n",
      "        [ 35.77052689, -78.67478180]]), tensor([[ 35.77033234, -78.67496490],\n",
      "        [ 35.77032852, -78.67496490],\n",
      "        [ 35.77032471, -78.67496490],\n",
      "        [ 35.77033615, -78.67494965],\n",
      "        [ 35.77031708, -78.67496490],\n",
      "        [ 35.77052307, -78.67480469],\n",
      "        [ 35.77052689, -78.67480469],\n",
      "        [ 35.77054977, -78.67474365],\n",
      "        [ 35.77055359, -78.67474365],\n",
      "        [ 35.77038956, -78.67491913]]), tensor([[ 35.77038956, -78.67491150],\n",
      "        [ 35.77038193, -78.67489624],\n",
      "        [ 35.77043152, -78.67482758],\n",
      "        [ 35.77047729, -78.67480469],\n",
      "        [ 35.77043915, -78.67488098],\n",
      "        [ 35.77046204, -78.67481232],\n",
      "        [ 35.77046585, -78.67480469],\n",
      "        [ 35.77046204, -78.67486572],\n",
      "        [ 35.77050018, -78.67484283],\n",
      "        [ 35.77061844, -78.67480469]]), tensor([[ 35.77038956, -78.67507935],\n",
      "        [ 35.77038574, -78.67508698],\n",
      "        [ 35.77045822, -78.67498016],\n",
      "        [ 35.77046967, -78.67496490],\n",
      "        [ 35.77032471, -78.67516327],\n",
      "        [ 35.77026367, -78.67524719],\n",
      "        [ 35.77026367, -78.67524719],\n",
      "        [ 35.77025986, -78.67525482],\n",
      "        [ 35.76979446, -78.67600250],\n",
      "        [ 35.76786041, -78.67800903]]), tensor([[ 35.76855087, -78.67734528],\n",
      "        [ 35.76839828, -78.67830658],\n",
      "        [ 35.76857758, -78.67803955],\n",
      "        [ 35.76758194, -78.67819214],\n",
      "        [ 35.77045822, -78.67499542],\n",
      "        [ 35.77025604, -78.67535400],\n",
      "        [ 35.77038956, -78.67517090],\n",
      "        [ 35.77047729, -78.67517090],\n",
      "        [ 35.77047729, -78.67516327],\n",
      "        [ 35.77047729, -78.67516327]]), tensor([[ 35.77041626, -78.67514038],\n",
      "        [ 35.77048874, -78.67515564],\n",
      "        [ 35.77077103, -78.67487335],\n",
      "        [ 35.77108765, -78.67544556],\n",
      "        [ 35.77093506, -78.67529297],\n",
      "        [ 35.77098465, -78.67461395],\n",
      "        [ 35.77070236, -78.67493439],\n",
      "        [ 35.77069092, -78.67492676],\n",
      "        [ 35.77059174, -78.67509460],\n",
      "        [ 35.77079010, -78.67467499]]), tensor([[ 35.77088165, -78.67463684],\n",
      "        [ 35.77087784, -78.67464447],\n",
      "        [ 35.77069473, -78.67484283],\n",
      "        [ 35.77049637, -78.67495728],\n",
      "        [ 35.77096176, -78.67449188],\n",
      "        [ 35.77098465, -78.67448425],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77110672, -78.67441559],\n",
      "        [ 35.77112579, -78.67436218],\n",
      "        [ 35.77114487, -78.67436981]]), tensor([[ 35.77072525, -78.67477417],\n",
      "        [ 35.77080917, -78.67466736],\n",
      "        [ 35.77111053, -78.67437744],\n",
      "        [ 35.77114487, -78.67436218],\n",
      "        [ 35.77101517, -78.67444611],\n",
      "        [ 35.77114868, -78.67430115],\n",
      "        [ 35.77098465, -78.67444611],\n",
      "        [ 35.77112579, -78.67432404],\n",
      "        [ 35.77096558, -78.67446899],\n",
      "        [ 35.77096176, -78.67446899]]), tensor([[ 35.77096939, -78.67446136],\n",
      "        [ 35.77096558, -78.67446899],\n",
      "        [ 35.77124786, -78.67420197],\n",
      "        [ 35.77115250, -78.67424011],\n",
      "        [ 35.77114105, -78.67429352],\n",
      "        [ 35.77123260, -78.67421722],\n",
      "        [ 35.77116394, -78.67433167],\n",
      "        [ 35.77116776, -78.67432404],\n",
      "        [ 35.77082443, -78.67456818],\n",
      "        [ 35.77087402, -78.67450714]]), tensor([[ 35.77082825, -78.67457581],\n",
      "        [ 35.77060318, -78.67481232],\n",
      "        [ 35.77060318, -78.67480469],\n",
      "        [ 35.77061462, -78.67478943],\n",
      "        [ 35.77048111, -78.67480469],\n",
      "        [ 35.77048492, -78.67479706],\n",
      "        [ 35.77062607, -78.67473602],\n",
      "        [ 35.77062988, -78.67472076],\n",
      "        [ 35.77034378, -78.67497253],\n",
      "        [ 35.77034378, -78.67496490]]), tensor([[ 35.77039337, -78.67492676],\n",
      "        [ 35.77024841, -78.67491150],\n",
      "        [ 35.77040482, -78.67491150],\n",
      "        [ 35.77036285, -78.67494202],\n",
      "        [ 35.77035904, -78.67494202],\n",
      "        [ 35.77037430, -78.67493439],\n",
      "        [ 35.77051163, -78.67498016],\n",
      "        [ 35.77043533, -78.67503357],\n",
      "        [ 35.77040100, -78.67509460],\n",
      "        [ 35.77059174, -78.67499542]]), tensor([[ 35.77057648, -78.67501831],\n",
      "        [ 35.77074814, -78.67480469],\n",
      "        [ 35.77075195, -78.67480469],\n",
      "        [ 35.77033234, -78.67523956],\n",
      "        [ 35.76976395, -78.67661285],\n",
      "        [ 35.76976776, -78.67661285],\n",
      "        [ 35.76945496, -78.67676544],\n",
      "        [ 35.77064896, -78.67494965],\n",
      "        [ 35.77083206, -78.67464447],\n",
      "        [ 35.76957703, -78.67652130]]), tensor([[ 35.77085876, -78.67462921],\n",
      "        [ 35.77106476, -78.67439270],\n",
      "        [ 35.77106476, -78.67439270],\n",
      "        [ 35.77106094, -78.67439270],\n",
      "        [ 35.77082062, -78.67466736],\n",
      "        [ 35.77082062, -78.67466736],\n",
      "        [ 35.77082062, -78.67467499],\n",
      "        [ 35.77104568, -78.67442322],\n",
      "        [ 35.77103806, -78.67442322],\n",
      "        [ 35.77103424, -78.67443085]]), tensor([[ 35.77091599, -78.67466736],\n",
      "        [ 35.77115250, -78.67434692],\n",
      "        [ 35.77104187, -78.67442322],\n",
      "        [ 35.77107620, -78.67437744],\n",
      "        [ 35.77107620, -78.67436981],\n",
      "        [ 35.77102280, -78.67443848],\n",
      "        [ 35.77111816, -78.67433929],\n",
      "        [ 35.77112198, -78.67433929],\n",
      "        [ 35.77112579, -78.67437744],\n",
      "        [ 35.77115250, -78.67430115]]), tensor([[ 35.77118301, -78.67427063],\n",
      "        [ 35.77115631, -78.67429352],\n",
      "        [ 35.77113724, -78.67433167],\n",
      "        [ 35.77104950, -78.67436981],\n",
      "        [ 35.77093887, -78.67451477],\n",
      "        [ 35.77103424, -78.67442322],\n",
      "        [ 35.77103043, -78.67443085],\n",
      "        [ 35.77097321, -78.67446136],\n",
      "        [ 35.77108383, -78.67436981],\n",
      "        [ 35.77092743, -78.67449951]]), tensor([[ 35.77090454, -78.67454529],\n",
      "        [ 35.77094650, -78.67449951],\n",
      "        [ 35.77095795, -78.67448425],\n",
      "        [ 35.77111053, -78.67433929],\n",
      "        [ 35.77123260, -78.67420959],\n",
      "        [ 35.77116394, -78.67429352],\n",
      "        [ 35.77098465, -78.67443848],\n",
      "        [ 35.77068329, -78.67485046],\n",
      "        [ 35.77071762, -78.67468262],\n",
      "        [ 35.77077866, -78.67460632]]), tensor([[ 35.77056122, -78.67478943],\n",
      "        [ 35.77055740, -78.67478943],\n",
      "        [ 35.77058029, -78.67480469],\n",
      "        [ 35.77058411, -78.67480469],\n",
      "        [ 35.77062607, -78.67475128],\n",
      "        [ 35.77341080, -78.67244720],\n",
      "        [ 35.77074814, -78.67465210],\n",
      "        [ 35.77065659, -78.67471313],\n",
      "        [ 35.77065659, -78.67471313],\n",
      "        [ 35.77072525, -78.67797852]]), tensor([[ 35.77045822, -78.67509460],\n",
      "        [ 35.77050018, -78.67502594],\n",
      "        [ 35.77050018, -78.67501831],\n",
      "        [ 35.77043915, -78.67505646],\n",
      "        [ 35.77043152, -78.67507172],\n",
      "        [ 35.77031326, -78.67522430],\n",
      "        [ 35.77030945, -78.67522430],\n",
      "        [ 35.77033234, -78.67519379],\n",
      "        [ 35.77033234, -78.67520142],\n",
      "        [ 35.77042007, -78.67507172]]), tensor([[ 35.77030945, -78.67521667],\n",
      "        [ 35.77040100, -78.67509460],\n",
      "        [ 35.77030945, -78.67521667],\n",
      "        [ 35.77028656, -78.67523956],\n",
      "        [ 35.77033997, -78.67514038],\n",
      "        [ 35.77030945, -78.67518616],\n",
      "        [ 35.77008057, -78.67547607],\n",
      "        [ 35.77023315, -78.67527008],\n",
      "        [ 35.77024460, -78.67525482],\n",
      "        [ 35.77027130, -78.67522430]]), tensor([[ 35.77037811, -78.67507172],\n",
      "        [ 35.77038193, -78.67506409],\n",
      "        [ 35.77041626, -78.67512512],\n",
      "        [ 35.77033234, -78.67514801],\n",
      "        [ 35.77037811, -78.67498016],\n",
      "        [ 35.77038193, -78.67497253],\n",
      "        [ 35.77035141, -78.67501068],\n",
      "        [ 35.77040863, -78.67491913],\n",
      "        [ 35.77061462, -78.67475891],\n",
      "        [ 35.77028656, -78.67498779]]), tensor([[ 35.77038193, -78.67495728],\n",
      "        [ 35.77039719, -78.67493439],\n",
      "        [ 35.77042770, -78.67488861],\n",
      "        [ 35.77025223, -78.67513275],\n",
      "        [ 35.76519012, -78.68005371],\n",
      "        [ 35.76483917, -78.68042755],\n",
      "        [ 35.76936722, -78.67556763],\n",
      "        [ 35.76448059, -78.68124390],\n",
      "        [ 35.77032089, -78.67495728],\n",
      "        [ 35.77032089, -78.67495728]]), tensor([[ 35.77037048, -78.67493439],\n",
      "        [ 35.77032089, -78.67496490],\n",
      "        [ 35.77033234, -78.67496490],\n",
      "        [ 35.77033997, -78.67496490],\n",
      "        [ 35.77029800, -78.67503357],\n",
      "        [ 35.77029037, -78.67503357],\n",
      "        [ 35.77034378, -78.67496490],\n",
      "        [ 35.77102280, -78.67441559],\n",
      "        [ 35.77101135, -78.67442322],\n",
      "        [ 35.77089691, -78.67454529]]), tensor([[ 35.77139282, -78.67413330],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77079010, -78.67465973],\n",
      "        [ 35.77078247, -78.67466736],\n",
      "        [ 35.77104187, -78.67446136],\n",
      "        [ 35.77085876, -78.67463684],\n",
      "        [ 35.77084732, -78.67465210],\n",
      "        [ 35.77104950, -78.67445374],\n",
      "        [ 35.77100754, -78.67446899],\n",
      "        [ 35.77100754, -78.67446899]]), tensor([[ 35.77105331, -78.67443848],\n",
      "        [ 35.77104950, -78.67443848],\n",
      "        [ 35.77103043, -78.67443085],\n",
      "        [ 35.77098083, -78.67448425],\n",
      "        [ 35.77122116, -78.67422485],\n",
      "        [ 35.77105713, -78.67436218],\n",
      "        [ 35.77114105, -78.67429352],\n",
      "        [ 35.77113724, -78.67429352],\n",
      "        [ 35.77120590, -78.67424011],\n",
      "        [ 35.77090073, -78.67463684]]), tensor([[ 35.77101898, -78.67441559],\n",
      "        [ 35.77111435, -78.67432404],\n",
      "        [ 35.77095032, -78.67459869],\n",
      "        [ 35.77115250, -78.67427826],\n",
      "        [ 35.77103424, -78.67440033],\n",
      "        [ 35.77120209, -78.67423248],\n",
      "        [ 35.77121735, -78.67421722],\n",
      "        [ 35.77081680, -78.67465973],\n",
      "        [ 35.77115250, -78.67427826],\n",
      "        [ 35.77086258, -78.67468262]]), tensor([[ 35.77122879, -78.67420959],\n",
      "        [ 35.77114868, -78.67428589],\n",
      "        [ 35.77115250, -78.67427826],\n",
      "        [ 35.77114868, -78.67427826],\n",
      "        [ 35.77115250, -78.67427826],\n",
      "        [ 35.77090073, -78.67462921],\n",
      "        [ 35.77114868, -78.67427826],\n",
      "        [ 35.77114487, -78.67428589],\n",
      "        [ 35.77115250, -78.67427826],\n",
      "        [ 35.77115631, -78.67427826]]), tensor([[ 35.77114487, -78.67428589],\n",
      "        [ 35.77114868, -78.67428589],\n",
      "        [ 35.77115631, -78.67427826],\n",
      "        [ 35.77090073, -78.67462921],\n",
      "        [ 35.77074432, -78.67478943],\n",
      "        [ 35.77108765, -78.67436981],\n",
      "        [ 35.77114868, -78.67428589],\n",
      "        [ 35.77074814, -78.67478943],\n",
      "        [ 35.77114868, -78.67428589],\n",
      "        [ 35.77114868, -78.67428589]]), tensor([[ 35.77114868, -78.67428589],\n",
      "        [ 35.77109909, -78.67436218],\n",
      "        [ 35.77106476, -78.67435455],\n",
      "        [ 35.77109146, -78.67436981],\n",
      "        [ 35.77114105, -78.67429352],\n",
      "        [ 35.77088928, -78.67463684],\n",
      "        [ 35.77114868, -78.67428589],\n",
      "        [ 35.77114868, -78.67428589],\n",
      "        [ 35.77087402, -78.67465973],\n",
      "        [ 35.77114105, -78.67429352]]), tensor([[ 35.77121735, -78.67422485],\n",
      "        [ 35.77114487, -78.67428589],\n",
      "        [ 35.77114868, -78.67428589],\n",
      "        [ 35.77114487, -78.67428589],\n",
      "        [ 35.77114487, -78.67429352],\n",
      "        [ 35.77114868, -78.67428589],\n",
      "        [ 35.77114105, -78.67429352],\n",
      "        [ 35.77114487, -78.67429352],\n",
      "        [ 35.77099228, -78.67454529],\n",
      "        [ 35.77114487, -78.67428589]]), tensor([[ 35.77114868, -78.67428589],\n",
      "        [ 35.77074814, -78.67478180],\n",
      "        [ 35.77114487, -78.67428589],\n",
      "        [ 35.77114487, -78.67429352],\n",
      "        [ 35.77107620, -78.67434692],\n",
      "        [ 35.77108765, -78.67432404],\n",
      "        [ 35.77103043, -78.67439270],\n",
      "        [ 35.77106094, -78.67435455],\n",
      "        [ 35.77108002, -78.67433929],\n",
      "        [ 35.77104568, -78.67437744]]), tensor([[ 35.77111053, -78.67430115],\n",
      "        [ 35.77105713, -78.67437744],\n",
      "        [ 35.77105713, -78.67437744],\n",
      "        [ 35.77098083, -78.67446136],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77067566, -78.67474365],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77068329, -78.67473602],\n",
      "        [ 35.77097702, -78.67442322]]), tensor([[ 35.77109909, -78.67436218],\n",
      "        [ 35.77100754, -78.67443848],\n",
      "        [ 35.77115250, -78.67431641],\n",
      "        [ 35.77112579, -78.67434692],\n",
      "        [ 35.77088165, -78.67456818],\n",
      "        [ 35.77119446, -78.67426300],\n",
      "        [ 35.77108002, -78.67440033],\n",
      "        [ 35.77181625, -78.67389679],\n",
      "        [ 35.77119064, -78.67426300],\n",
      "        [ 35.77106476, -78.67445374]]), tensor([[ 35.77109909, -78.67438507],\n",
      "        [ 35.77113724, -78.67433167],\n",
      "        [ 35.77100372, -78.67448425],\n",
      "        [ 35.77098846, -78.67451477],\n",
      "        [ 35.77103806, -78.67440033],\n",
      "        [ 35.77100372, -78.67448425],\n",
      "        [ 35.77100372, -78.67448425],\n",
      "        [ 35.77104568, -78.67443085],\n",
      "        [ 35.77059937, -78.67478180],\n",
      "        [ 35.77043152, -78.67475128]]), tensor([[ 35.77079391, -78.67463684],\n",
      "        [ 35.77078629, -78.67465210],\n",
      "        [ 35.77070999, -78.67469025],\n",
      "        [ 35.77070999, -78.67469788],\n",
      "        [ 35.77061081, -78.67476654],\n",
      "        [ 35.77061081, -78.67476654],\n",
      "        [ 35.77056503, -78.67482758],\n",
      "        [ 35.77074432, -78.67465973],\n",
      "        [ 35.77074432, -78.67465210],\n",
      "        [ 35.77074432, -78.67465210]]), tensor([[ 35.77035141, -78.67516327],\n",
      "        [ 35.77035141, -78.67515564],\n",
      "        [ 35.77034378, -78.67517090],\n",
      "        [ 35.77040482, -78.67510986],\n",
      "        [ 35.77039719, -78.67512512],\n",
      "        [ 35.77039337, -78.67513275],\n",
      "        [ 35.77047348, -78.67506409],\n",
      "        [ 35.77047729, -78.67505646],\n",
      "        [ 35.77046204, -78.67502594],\n",
      "        [ 35.77045059, -78.67504883]]), tensor([[ 35.77044678, -78.67504883],\n",
      "        [ 35.77045441, -78.67504120],\n",
      "        [ 35.77043915, -78.67506409],\n",
      "        [ 35.77035141, -78.67517090],\n",
      "        [ 35.77034760, -78.67517853],\n",
      "        [ 35.77038193, -78.67513275],\n",
      "        [ 35.77037048, -78.67515564],\n",
      "        [ 35.77038574, -78.67513275],\n",
      "        [ 35.77033615, -78.67517853],\n",
      "        [ 35.77042770, -78.67505646]]), tensor([[ 35.77040863, -78.67508698],\n",
      "        [ 35.77040482, -78.67509460],\n",
      "        [ 35.77038956, -78.67510986],\n",
      "        [ 35.77039719, -78.67510223],\n",
      "        [ 35.77030563, -78.67521667],\n",
      "        [ 35.77031708, -78.67519379],\n",
      "        [ 35.77032852, -78.67516327],\n",
      "        [ 35.77030182, -78.67520142],\n",
      "        [ 35.77003860, -78.67547607],\n",
      "        [ 35.77027512, -78.67521667]]), tensor([[ 35.76951981, -78.67630768],\n",
      "        [ 35.77017212, -78.67527771],\n",
      "        [ 35.77016449, -78.67527771],\n",
      "        [ 35.77017975, -78.67526245],\n",
      "        [ 35.77044296, -78.67489624],\n",
      "        [ 35.77036667, -78.67498016],\n",
      "        [ 35.77038193, -78.67496490],\n",
      "        [ 35.77034378, -78.67499542],\n",
      "        [ 35.77035141, -78.67499542],\n",
      "        [ 35.76969528, -78.67520905]]), tensor([[ 35.77060699, -78.67479706],\n",
      "        [ 35.77024460, -78.67514038],\n",
      "        [ 35.76922607, -78.67557526],\n",
      "        [ 35.76565933, -78.68083954],\n",
      "        [ 35.77031708, -78.67496490],\n",
      "        [ 35.77032471, -78.67496490],\n",
      "        [ 35.77033234, -78.67495728],\n",
      "        [ 35.77039337, -78.67491150],\n",
      "        [ 35.77039337, -78.67491150],\n",
      "        [ 35.77039719, -78.67491150]]), tensor([[ 35.77028656, -78.67502594],\n",
      "        [ 35.77028275, -78.67503357],\n",
      "        [ 35.77029037, -78.67502594],\n",
      "        [ 35.77033615, -78.67496490],\n",
      "        [ 35.77032852, -78.67496490],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77205276, -78.67359161],\n",
      "        [ 35.77096939, -78.67448425],\n",
      "        [ 35.77212906, -78.67355347]]), tensor([[ 35.77219391, -78.67346191],\n",
      "        [ 35.77221298, -78.67344666],\n",
      "        [ 35.77101898, -78.67442322],\n",
      "        [ 35.77095795, -78.67450714],\n",
      "        [ 35.77100372, -78.67444611],\n",
      "        [ 35.77079773, -78.67466736],\n",
      "        [ 35.77093887, -78.67456055],\n",
      "        [ 35.77109528, -78.67440033],\n",
      "        [ 35.77088165, -78.67460632],\n",
      "        [ 35.77088165, -78.67459869]]), tensor([[ 35.77128220, -78.67421722],\n",
      "        [ 35.77096176, -78.67453766],\n",
      "        [ 35.77108383, -78.67440033],\n",
      "        [ 35.77119827, -78.67430115],\n",
      "        [ 35.77096558, -78.67451477],\n",
      "        [ 35.77114105, -78.67436218],\n",
      "        [ 35.77095032, -78.67447662],\n",
      "        [ 35.77116394, -78.67430115],\n",
      "        [ 35.77116776, -78.67430115],\n",
      "        [ 35.77109909, -78.67437744]]), tensor([[ 35.77116013, -78.67429352],\n",
      "        [ 35.77112579, -78.67434692],\n",
      "        [ 35.77118683, -78.67426300],\n",
      "        [ 35.77119446, -78.67426300],\n",
      "        [ 35.77111435, -78.67436218],\n",
      "        [ 35.77097702, -78.67453766],\n",
      "        [ 35.77098465, -78.67447662],\n",
      "        [ 35.77096558, -78.67449188],\n",
      "        [ 35.77114487, -78.67432404],\n",
      "        [ 35.77102280, -78.67446136]]), tensor([[ 35.77101898, -78.67446136],\n",
      "        [ 35.77108383, -78.67439270],\n",
      "        [ 35.77109909, -78.67436981],\n",
      "        [ 35.77066040, -78.67477417],\n",
      "        [ 35.77060699, -78.67485046],\n",
      "        [ 35.77081299, -78.67459869],\n",
      "        [ 35.77081680, -78.67460632],\n",
      "        [ 35.77086639, -78.67454529],\n",
      "        [ 35.77087021, -78.67454529],\n",
      "        [ 35.77061462, -78.67476654]]), tensor([[ 35.77059555, -78.67481232],\n",
      "        [ 35.77064896, -78.67474365],\n",
      "        [ 35.77066803, -78.67475891],\n",
      "        [ 35.77066803, -78.67475891],\n",
      "        [ 35.77073669, -78.67465973],\n",
      "        [ 35.77074432, -78.67465973],\n",
      "        [ 35.77074051, -78.67465973],\n",
      "        [ 35.77041626, -78.67510223],\n",
      "        [ 35.77044678, -78.67510986],\n",
      "        [ 35.77045441, -78.67510223]]), tensor([[ 35.77040863, -78.67503357],\n",
      "        [ 35.77040482, -78.67503357],\n",
      "        [ 35.77043915, -78.67488861],\n",
      "        [ 35.77053070, -78.67478180],\n",
      "        [ 35.77027893, -78.67504120],\n",
      "        [ 35.77054977, -78.67473602],\n",
      "        [ 35.77030182, -78.67510986],\n",
      "        [ 35.77032471, -78.67496490],\n",
      "        [ 35.77032471, -78.67496490],\n",
      "        [ 35.77032852, -78.67496490]]), tensor([[ 35.77032852, -78.67496490],\n",
      "        [ 35.77032852, -78.67496490],\n",
      "        [ 35.77035141, -78.67494202],\n",
      "        [ 35.77069473, -78.67463684],\n",
      "        [ 35.77056122, -78.67473602],\n",
      "        [ 35.77044296, -78.67483521],\n",
      "        [ 35.77043533, -78.67484283],\n",
      "        [ 35.77043152, -78.67485046],\n",
      "        [ 35.77042007, -78.67485809],\n",
      "        [ 35.77063751, -78.67475128]]), tensor([[ 35.77045822, -78.67487335],\n",
      "        [ 35.77046204, -78.67486572],\n",
      "        [ 35.77045822, -78.67486572],\n",
      "        [ 35.77045441, -78.67486572],\n",
      "        [ 35.77036667, -78.67504120],\n",
      "        [ 35.77036285, -78.67504120],\n",
      "        [ 35.77037048, -78.67503357],\n",
      "        [ 35.77044296, -78.67502594],\n",
      "        [ 35.77059937, -78.67488098],\n",
      "        [ 35.77048492, -78.67494965]]), tensor([[ 35.77040863, -78.67505646],\n",
      "        [ 35.77043915, -78.67501831],\n",
      "        [ 35.77029419, -78.67519379],\n",
      "        [ 35.77024841, -78.67526245],\n",
      "        [ 35.77024078, -78.67527008],\n",
      "        [ 35.77022552, -78.67528534],\n",
      "        [ 35.76866150, -78.67805481],\n",
      "        [ 35.76866531, -78.67803955],\n",
      "        [ 35.76939392, -78.67637634],\n",
      "        [ 35.76919174, -78.67702484]]), tensor([[ 35.76834106, -78.67726898],\n",
      "        [ 35.77017212, -78.67531586],\n",
      "        [ 35.77024460, -78.67535400],\n",
      "        [ 35.77035904, -78.67522430],\n",
      "        [ 35.77050018, -78.67530060],\n",
      "        [ 35.77036667, -78.67520142],\n",
      "        [ 35.77043152, -78.67511749],\n",
      "        [ 35.77056122, -78.67523956],\n",
      "        [ 35.77056503, -78.67530060],\n",
      "        [ 35.77048111, -78.67515564]]), tensor([[ 35.77048111, -78.67515564],\n",
      "        [ 35.77058792, -78.67510986],\n",
      "        [ 35.77057266, -78.67504120],\n",
      "        [ 35.77068710, -78.67494965],\n",
      "        [ 35.77077103, -78.67485046],\n",
      "        [ 35.77084351, -78.67473602],\n",
      "        [ 35.77092361, -78.67472076],\n",
      "        [ 35.77092361, -78.67471313],\n",
      "        [ 35.77091599, -78.67472076],\n",
      "        [ 35.77103806, -78.67630005]]), tensor([[ 35.77090454, -78.67597198],\n",
      "        [ 35.77062225, -78.67503357],\n",
      "        [ 35.77088928, -78.67463684],\n",
      "        [ 35.77094269, -78.67456055],\n",
      "        [ 35.77097321, -78.67443085],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77072144, -78.67469788],\n",
      "        [ 35.77111816, -78.67436218],\n",
      "        [ 35.77114487, -78.67430115],\n",
      "        [ 35.77099991, -78.67443848]]), tensor([[ 35.77097321, -78.67446136],\n",
      "        [ 35.77096558, -78.67446899],\n",
      "        [ 35.77109528, -78.67431641],\n",
      "        [ 35.77109146, -78.67434692],\n",
      "        [ 35.77124405, -78.67420959],\n",
      "        [ 35.77108002, -78.67444611],\n",
      "        [ 35.77064896, -78.67478180],\n",
      "        [ 35.77064896, -78.67478180],\n",
      "        [ 35.77089310, -78.67456055],\n",
      "        [ 35.77112198, -78.67427826]]), tensor([[ 35.77072144, -78.67469788],\n",
      "        [ 35.77111816, -78.67428589],\n",
      "        [ 35.77110291, -78.67430115]])]\n",
      "Number of batches processed: 84\n",
      "Number of predictions made: 833\n",
      "Total image paths processed: 84\n"
     ]
    }
   ],
   "source": [
    "lat_lon_model_with_3digits = LatLonModelWith3Digits()\n",
    "lat_lon_model_with_3digits.load_state_dict(torch.load('ffnn_location_no_shortened_digits.pt', weights_only=True))  # Load the saved weights\n",
    "lat_lon_model_with_3digits.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Making predictions and saving them\n",
    "annotation_file = 'test_5/New_2BBox_Each_Class_Annotation.json'\n",
    "found_image_dir = \"test_5_found_image\"\n",
    "lat_lon_dataset_with_3digits = COCODataset(annotation_file, found_image_dir)\n",
    "lat_lon_test_dataloader = DataLoader(lat_lon_dataset_with_3digits, batch_size=10, shuffle=False)\n",
    "\n",
    "print(f\"Total images in the dataset: {len(lat_lon_dataset_with_3digits)}\")\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "image_paths = []  # Uncommented to track image paths\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in lat_lon_test_dataloader:\n",
    "        # Predict outputs\n",
    "        outputs = lat_lon_model_with_3digits(inputs)\n",
    "        \n",
    "        # print(outputs)\n",
    "        all_predictions.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        \n",
    "        image_paths.append(targets)  \n",
    "print(all_predictions)\n",
    "flat_predictions = torch.cat(all_predictions, dim=0).cpu().numpy()  # Converts to numpy array\n",
    "flat_image_paths = [path for batch in image_paths for path in batch]  # Flatten image paths if batched\n",
    "\n",
    "# Check if all images were processed\n",
    "print(f\"Number of batches processed: {len(all_predictions)}\")\n",
    "print(f\"Number of predictions made: {sum([len(batch) for batch in all_predictions])}\")\n",
    "# print(all_targets)\n",
    "# Optionally, print or check specific outputs\n",
    "# for i, prediction in enumerate(all_predictions):\n",
    "#     print(f\"Batch {i}: {prediction}\")\n",
    "\n",
    "data = {\n",
    "    \"image_path\" : flat_image_paths,\n",
    "    \"predicted_lat\" : flat_predictions[:, 0],\n",
    "    \"predicted_lon\": flat_predictions[:, 1]\n",
    "}\n",
    "\n",
    "# Creating a csv file for lat and lon with 3 digits \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"predictions_for_testing/new_lat_lon_model_with_3digits.csv\", index=False, float_format=\"%.8f\")\n",
    "\n",
    "print(f\"Total image paths processed: {len(image_paths)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
